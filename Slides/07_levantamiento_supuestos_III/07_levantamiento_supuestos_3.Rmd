---
title: "(Heterocedasticidad & autocorrelaci√≥n) + Bootstrap"
subtitle: "Econometr√≠a I"
author: "Paula Pereda (ppereda@correo.um.edu.uy)"
date: "7 de octubre de 2021"
output:
  xaringan::moon_reader:
    css: ['default', 'metropolis', 'metropolis-fonts', 'my-css.css']
    # self_contained: true
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
---
class: inverse, middle

```{r Setup, include = F}

library(broom)
library(latex2exp)
library(ggplot2)
library(ggthemes)
library(viridis)
library(extrafont)
library(gridExtra)
library(magrittr)
library(knitr)
library(parallel)  
library(tibble)
library(extrafont)
library(kableExtra)
  
# Define pink color
red_pink <- "#e64173"
grey_light <- "grey70"
grey_mid <- "grey50"
grey_dark <- "grey20"
# Dark slate grey: #314f4f
# Knitr options
opts_chunk$set(
  comment = ">",
  fig.align = "center",
  fig.height = 7,
  fig.width = 10.5,
  warning = F,
  message = F
)

# A blank theme for ggplot
theme_empty <- theme_bw() + theme(
  line = element_blank(),
  rect = element_blank(),
  strip.text = element_blank(),
  axis.text = element_blank(),
  plot.title = element_blank(),
  axis.title = element_blank(),
  plot.margin = structure(c(0, 0, -0.5, -1), unit = "lines", valid.unit = 3L, class = "unit"),
  legend.position = "none"
)
theme_simple <- theme_bw() + theme(
  line = element_blank(),
  panel.grid = element_blank(),
  rect = element_blank(),
  strip.text = element_blank(),
  axis.text.x = element_text(size = 18),
  axis.text.y = element_blank(),
  axis.ticks = element_blank(),
  plot.title = element_blank(),
  axis.title = element_blank(),
  # plot.margin = structure(c(0, 0, -1, -1), unit = "lines", valid.unit = 3L, class = "unit"),
  legend.position = "none"
)
theme_axes_math <- theme_void() + theme(
  axis.title = element_text(size = 22),
  axis.title.x = element_text(hjust = .95, margin = margin(0.15, 0, 0, 0, unit = "lines")),
  axis.title.y = element_text(vjust = .95, margin = margin(0, 0.15, 0, 0, unit = "lines")),
  axis.line = element_line(
    color = "grey70",
    size = 0.25,
    arrow = arrow(angle = 30, length = unit(0.15, "inches")
  )),
  plot.margin = structure(c(1, 0, 1, 0), unit = "lines", valid.unit = 3L, class = "unit"),
  legend.position = "none"
)
theme_axes_serif <- theme_void() + theme(
  axis.title = element_text(size = 22),
  axis.title.x = element_text(hjust = .95, margin = margin(0.15, 0, 0, 0, unit = "lines")),
  axis.title.y = element_text(vjust = .95, margin = margin(0, 0.15, 0, 0, unit = "lines")),
  axis.line = element_line(
    color = "grey70",
    size = 0.25,
    arrow = arrow(angle = 30, length = unit(0.15, "inches")
  )),
  plot.margin = structure(c(1, 0, 1, 0), unit = "lines", valid.unit = 3L, class = "unit"),
  legend.position = "none"
)
theme_axes <- theme_void() + theme(
  axis.title = element_text(size = 18),
  axis.title.x = element_text(hjust = .95, margin = margin(0.15, 0, 0, 0, unit = "lines")),
  axis.title.y = element_text(vjust = .95, margin = margin(0, 0.15, 0, 0, unit = "lines")),
  axis.line = element_line(
    color = grey_light,
    size = 0.25,
    arrow = arrow(angle = 30, length = unit(0.15, "inches")
  )),
  plot.margin = structure(c(1, 0, 1, 0), unit = "lines", valid.unit = 3L, class = "unit"),
  legend.position = "none"
)
```


# Heterocedasticidad
---


Escribamos nuestros **supuestos actuales**

--

1. Nuestra muestra (los $x_k$'s y la $y_i$) fueron .hi[extra√≠das aleatoriamente] de la poblaci√≥n.

--

2. $y$ es una .hi[funci√≥n lineal] de los $\beta_k$'s y $u_i$.

--

3. No hay .hi[multicolinealidad perfecta] en nuestra muestra.

--

4. Las variables explicativas son .hi[exogenas]: $\mathop{\boldsymbol{E}}\left[ u \middle| X \right] = 0 \left(\implies \mathop{\boldsymbol{E}}\left[ u \right] = 0\right)$.

--

5. Los errores tiene .hi[varianza constante] $\sigma^2$ y .hi[cero covarianza], _por ejemplo_,
  - $\mathop{\boldsymbol{E}}\left[ u_i^2 \middle| X \right] = \mathop{\text{Var}} \left( u_i \middle| X \right) = \sigma^2 \implies \mathop{\text{Var}} \left( u_i \right) = \sigma^2$
  - $\mathop{\text{Cov}} \left( u_i, \, u_j \middle| X \right) = \mathop{\boldsymbol{E}}\left[ u_i u_j \middle| X \right] = 0$ for $i\neq j$

--

6. Los errores provienen de una distribuci√≥n .hi[Normal], _por ejemplo_, $u_i \overset{\text{iid}}{\sim} \mathop{\text{N}}\left( 0, \sigma^2 \right)$.

---

Nos vamos a enfocar en el supuesto \#5:

> 5\. Los errores tiene .hi[varianza constante] $\sigma^2$ y .hi[cero covarianza], 
> _por ejemplo_,
> - $\mathop{\boldsymbol{E}}\left[ u_i^2 \middle| X \right] = \mathop{\text{Var}} \left( u_i \middle| X \right) = \sigma^2 \implies \mathop{\text{Var}} \left( u_i \right) = \sigma^2$
> - $\mathop{\text{Cov}} \left( u_i, \, u_j \middle| X \right) = \mathop{\boldsymbol{E}}\left[ u_i u_j \middle| X \right] = 0$ para $i\neq j$

--

Espec√≠ficamente, nos enfocaremos en el supuesto de .hi[varianza constante] (tambi√©n conocido como *homocedasticidad*).

--

**Violaci√≥n del supuesto:**

.hi[Heterocedasticidad:] $\mathop{\text{Var}} \left( u_i \right) = \sigma^2_i$ y $\sigma^2_i \neq \sigma^2_j$ para algunos $i\neq j$.

--

En otras palabras: nuestros errores tienen distintas varianzas.

---

Ejemplo cl√°sico de heterocedasticidad: el embudo

La varianza de $u$ aumenta con $x$

```{R, het ex1, dev = "svg", echo = F, fig.height = 5.5}
set.seed(12345)
ggplot(data = tibble(
  x = runif(1e3, -3, 3),
  e = rnorm(1e3, 0, sd = 4 + 1.5 * x)
), aes(x = x, y = e)) +
geom_point(color = "darkslategrey", size = 2.75, alpha = 0.5) +
labs(x = "x", y = "u") +
theme_axes_math
```
---

Otro ejemplo de heterocedasticidad: (¬ødoble embudo?)

Varianza de $u$ aumentando en los extremos de $x$

```{R, het ex2 , dev = "svg", echo = F, fig.height = 5.5}
set.seed(12345)
ggplot(data = tibble(
  x = runif(1e3, -3, 3),
  e = rnorm(1e3, 0, sd = 2 + x^2)
), aes(x = x, y = e)) +
geom_point(color = "darkslategrey", size = 2.75, alpha = 0.5) +
labs(x = "x", y = "u") +
theme_axes_math
```
---

Otra ejemplo de heterocedasticidad:

Diferentes variancias de $u$ por grupo

```{R, het ex3 , dev = "svg", echo = F, fig.height = 5.5}
set.seed(12345)
ggplot(data = tibble(
  g = sample(c(F,T), 1e3, replace = T),
  x = runif(1e3, -3, 3),
  e = rnorm(1e3, 0, sd = 0.5 + 2 * g)
), aes(x = x, y = e, color = g, shape = g, alpha = g)) +
geom_point(size = 2.75) +
scale_color_manual(values = c("darkslategrey", red_pink)) +
scale_shape_manual(values = c(16, 1)) +
scale_alpha_manual(values = c(0.5, 0.8)) +
labs(x = "x", y = "u") +
theme_axes_math
```
---

La .hi[heterocedasticidad] est√° presente donde la varianza de $u$ cambia con cualquier combinaci√≥n de nuestras variables explicativas $x_1$, a trav√©s de $x_k$ (por lo tanto, $X$).

--

(Muy com√∫n en la pr√°ctica)

---

## Consecuencias

Entonces... ¬øcu√°les son las consecuencias de la heterocedasticidad? ¬øSesgo? ¬øIneficiencia?

Primero, chequeamos si tiene consecuencias para la insesgadez de MCO.

--

**Recordemos<sub>1</sub>:** MCO siendo insesgado significa $\mathop{\boldsymbol{E}}\left[ \hat{\beta}_k \middle| X \right] = \beta_k$ para todos $k$.

--

**Recordemos<sub>2</sub>:** previamente demostramos que $\hat{\beta}_1 = \dfrac{\sum_i\left(y_i-\overline{y}\right)\left(x_i-\overline{x}\right)}{\sum_i\left(x_i -\overline{x}\right)^2}$

--

Nos ayudar√° escribir el estimador como

$$ \hat{\beta}_1 = \beta_1 + \dfrac{\sum_i \left( x_i - \overline{x} \right) u_i}{\sum_i \left( x_i - \overline{x} \right)^2} $$
---

**Prueba:** Asumiendo $y_i = \beta_0 + \beta_1 x_i + u_i$

$$
\begin{aligned}
  \hat{\beta}_1
  &= \dfrac{\sum_i\left(y_i-\overline{y}\right)\left(x_i-\overline{x}\right)}{\sum_i\left(x_i -\overline{x}\right)^2} \\
  &= \dfrac{\sum_i\left(\left[ \beta_0 + \beta_1 x_i + u_i \right]- \left[ \beta_0 + \beta_1 \overline{x} + \overline{u} \right] \right)\left(x_i-\overline{x}\right)}{\sum_i\left(x_i -\overline{x}\right)^2} \\
  &= \dfrac{\sum_i\left(\beta_1 \left[ x_i - \overline{x} \right] + \left[u_i - \overline{u}\right]  \right)\left(x_i-\overline{x}\right)}{\sum_i\left(x_i -\overline{x}\right)^2} \\
  &= \dfrac{\sum_i\left(\beta_1 \left[ x_i - \overline{x} \right]^2 + \left[ x_i - \overline{x} \right] \left[u_i - \overline{u}\right]\right)}{\sum_i\left(x_i -\overline{x}\right)^2} \\
  &= \beta_1 + \dfrac{\sum_i\left(x_i - \overline{x}\right) \left(u_i - \overline{u}\right)}{\sum_i\left(x_i -\overline{x}\right)^2}
\end{aligned}
$$
---

$$
\begin{aligned}
  \hat{\beta}_1
  &= \cdots = \beta_1 + \dfrac{\sum_i\left(x_i - \overline{x}\right) \left(u_i - \overline{u}\right)}{\sum_i\left(x_i -\overline{x}\right)^2} \\
  &= \beta_1 + \dfrac{\sum_i\left(x_i - \overline{x}\right) u_i - \overline{u} \sum_i\left(x_i - \overline{x}\right)}{\sum_i\left(x_i -\overline{x}\right)^2} \\
  &= \beta_1 + \dfrac{\sum_i\left(x_i - \overline{x}\right) u_i - \overline{u} \left(\sum_i x_i - \sum_i \overline{x}\right)}{\sum_i\left(x_i -\overline{x}\right)^2} \\
  &= \beta_1 + \dfrac{\sum_i\left(x_i - \overline{x}\right) u_i - \overline{u} \left(\sum_i x_i - n \overline{x}\right)}{\sum_i\left(x_i -\overline{x}\right)^2} \\
  &= \beta_1 + \dfrac{\sum_i\left(x_i - \overline{x}\right) u_i - \overline{u} \color{#e64173}{\left(\sum_i x_i - \sum_i x_i\right)}}{\sum_i\left(x_i -\overline{x}\right)^2} \\
  &= \beta_1 + \dfrac{\sum_i\left(x_i - \overline{x}\right) u_i}{\sum_i\left(x_i -\overline{x}\right)^2} \quad \text{üòÖ}
\end{aligned}
$$
---

## Consecuencias: Sesgo

Ahora queremos saber si la heterocedasticidad sesga el estimador de MCO para $\beta_1$.

--

$$
\begin{aligned}
  \mathop{\boldsymbol{E}}\left[ \hat{\beta}_1 \middle| X \right]
  &= \mathop{\boldsymbol{E}}\left[ \beta_1 + \dfrac{\sum_i\left(x_i - \overline{x}\right) u_i}{\sum_i\left(x_i -\overline{x}\right)^2} \middle| X \right] \\[0.5em]
  &= \beta_1 + \mathop{\boldsymbol{E}}\left[ \dfrac{\sum_i\left(x_i - \overline{x}\right) u_i}{\sum_i\left(x_i -\overline{x}\right)^2} \middle| X \right] \\[0.5em]
  &= \beta_1 + \dfrac{\sum_i\left(x_i - \overline{x}\right)}{\sum_i\left(x_i -\overline{x}\right)^2} \color{#e64173}{\underbrace{\mathop{\boldsymbol{E}}\left[ u_i \middle| X \right]}_{=0}} \\[0.5em]
  &= \beta_1
\end{aligned}
$$

--
.hi[MCO todav√≠a es insesgado] para los $\beta_k$.
---

## Consecuencias: Eficiencia

La eficiencia e inferencia de MCO no sobreviven la heterocedasticidad.

- En la presencia de heterocedasticidad, MCO ya no es .hi[MELI].

--
- Ser√≠a m√°s informativo (eficiente) .hi[ponderar las observaciones] inversamente a la varianza de su $u_i$.

  - Reducci√≥n de peso de alta variaci√≥n $u_i$'s (demasiado ruidoso para aprender mucho).

  - Observaciones al alza con baja varianza $u_i$ 's (m√°s confiable).

  - Ahora tienes la idea de m√≠nimos cuadrados ponderados (MCP)

---

## Consecuencias: Inferencia

Los .hi[errores est√°ndar de MCO est√°n sesgados] en presencia de heterocedasticidad.

- Intervalos de confianza incorrectos

- Problemas para el testeo de hipotesis (tanto $t$ como $F$ tests)

--

- Es dif√≠cil aprender sin inferencia.
---

## Soluciones

1. **Tests** para determinar si hay presencia de heterocedasticidad.

2. **Remedios** para (1) la eficiencia y (2) la inferencia

---
layout: true
# Testeando la heterocedasticidad

---
class: inverse, middle

---

Si bien *podr√≠amos* tener soluciones para la heterocedasticidad, la eficiencia de nuestros estimadores depende de si la heterocedasticidad est√° presente o no.

1. **Goldfeld-Quandt test**

1. **Breusch-Pagan test**

1. **White test**

--

Cada una de estas pruebas se centra en el hecho de que podemos .hi[usar el residuo de MCO] $\color{#e64173}{e_i}$ .hi[para estimar la perturbaci√≥n de la poblaci√≥n] $\color{#e64173}{u_i}$.


---
layout: true
# Testeando la heterocedasticidad
## El test de Goldfeld-Quandt
---

Se centra en un tipo espec√≠fico de heterocedasticidad: si la varianza de $u_i$ difiere .hi[entre dos grupos]. <sup>‚Ä†</sup>

¬øRecuerda c√≥mo usamos nuestros residuos para estimar $\sigma ^ 2$?

$$s ^ 2 = \dfrac {\text {SCE}} {n-1} = \dfrac {\sum_i e_i ^ 2} {n-1}$$

Usaremos esta misma idea para determinar si hay evidencia de que nuestros dos grupos difieren en las variaciones de sus perturbaciones, comparando efectivamente $s ^ 2_1$ y $s ^ 2_2$ de nuestros dos grupos.

.note[[‚Ä†]: La prueba G-Q fue una de las primeras pruebas de heterocedasticidad (1965).]

---

Operacionalmente,

.pseudocode-small[

1. Ordenamos las observaciones por $x$

2. Dividimos los datos en dos grupos de tama√±o n.super[‚≠ë]
  - G<sub>1</sub>: El primer tercio
  - G<sub>2</sub>: El √∫ltimo tercio

3. Corremos regresiones separadas de $y$ en $x$ para G.sub[1] y G.sub[2]

4. Nos quedamos con SCE.sub[1] y SCE.sub[2]

5. Calculamos el estad√≠stico del test G-Q

]
---

El estad√≠stico del test G-Q

$$ F_{\left(n^{\star}-k,\, n^{\star}-k\right)} = \dfrac{\text{SCE}_2/(n^\star-k)}{\text{SCE}_1/(n^\star-k)} = \dfrac{\text{SCE}_2}{\text{SCE}_1} $$

sigue una distribuci√≥n $F$ (bajo la hip√≥tesis nula) con $n^{\star}-k$ y $n^{\star}-k$ grados de libertad.<sup>‚Ä†</sup>

--

**Notas**

- El test G-Q requiere que los errores sigan una distribuci√≥n normal.
- G-Q asuma un tipo o forma muy espec√≠fica de heterocedasticidad.
- Funciona muy bien si conocemos la forma potencia de la heterocedasticidad.

.footnote[
[‚Ä†]: Goldfeld y Quandt sugirieron $n^{\star}$ de $(3/8)n$. $k$ da el n√∫mero de par√°metros estimados (_por ejemplo_, $\hat{\beta}_j$'s).
]
---

```{R, gq1a, echo = F, dev = "svg", fig.height = 4}
set.seed(12345)
# Data
gq_df <- tibble(
  x = runif(1e3, -3, 3),
  e = rnorm(1e3, 0, sd = 4 + 1.5 * x),
  y = 1 + 3 * x + e
)
# Quantiles
gq_x <- quantile(gq_df$x, probs = c(3/8, 5/8))
# Regressions
sse1 <- lm(y ~ x, data = subset(gq_df, x < gq_x[1])) %>%
  residuals() %>% magrittr::raise_to_power(2) %>% sum()
sse2 <- lm(y ~ x, data = subset(gq_df, x > gq_x[2])) %>%
  residuals() %>% magrittr::raise_to_power(2) %>% sum()
ggplot(data = gq_df, aes(x = x, y = e)) +
geom_point(color = "darkslategrey", size = 2.75, alpha = 0.5) +
labs(x = "x", y = "u") +
theme_axes_math
```
---

```{R, gq1b, echo = F, dev = "svg", fig.height = 4}
ggplot(data = gq_df, aes(
  x = x, y = e,
  color = cut(x, c(-Inf, gq_x, Inf)),
  alpha = cut(x, c(-Inf, gq_x, Inf)),
  shape = cut(x, c(-Inf, gq_x, Inf))
)) +
geom_vline(
  xintercept = gq_x,
  color = grey_mid,
  size = 0.25
) +
geom_point(size = 2.75) +
labs(x = "x", y = "u") +
scale_color_manual(values = c("darkslategrey", grey_mid, red_pink)) +
scale_shape_manual(values = c(19, 1, 19)) +
scale_alpha_manual(values = c(0.5, 0.8, 0.6)) +
theme_axes_math
```

$F_{375,\,375} = \dfrac{\color{#e64173}{\text{SCE}_2 = `r format(round(sse2, 1), nsmall = 0L, big.mark = ",")`}}{\color{#314f4f}{\text{SCE}_1 = `r format(round(sse1, 1), nsmall = 0L, big.mark = ",")`}} \approx `r format(round(sse2/sse1, 1), nsmall = 0L, big.mark = ",")` \implies$ *p*-valor $< 0.001$

$\therefore$ Rechazamos H.sub[0]: $\sigma^2_1 = \sigma^2_2$ y concluimos que hay evidencia estad√≠sticamente significativa de heterocedasticidad.
---

El problema...
---

```{R, gq2, echo = F, dev = "svg", fig.height = 4}
set.seed(12345)
# Data
gq2_df <- tibble(
  x = runif(1e3, -3, 3),
  e = rnorm(1e3, 0, sd = 2 + x^2),
  y = 1 + 3 * x + e
)
# Quantiles
gq_x <- quantile(gq2_df$x, probs = c(3/8, 5/8))
# Regressions
sse1b <- lm(y ~ x, data = subset(gq2_df, x < gq_x[1])) %>%
  residuals() %>% raise_to_power(2) %>% sum()
sse2b <- lm(y ~ x, data = subset(gq2_df, x > gq_x[2])) %>%
  residuals() %>% raise_to_power(2) %>% sum()
ggplot(data = gq2_df, aes(
  x = x, y = e,
  color = cut(x, c(-Inf, gq_x, Inf)),
  alpha = cut(x, c(-Inf, gq_x, Inf)),
  shape = cut(x, c(-Inf, gq_x, Inf))
)) +
geom_vline(
  xintercept = gq_x,
  color = grey_mid,
  size = 0.25
) +
geom_point(size = 2.75) +
labs(x = "x", y = "u") +
scale_color_manual(values = c("darkslategrey", grey_mid, red_pink)) +
scale_shape_manual(values = c(19, 1, 19)) +
scale_alpha_manual(values = c(0.5, 0.8, 0.6)) +
theme_axes_math
```

$F_{375,\,375} = \dfrac{\color{#e64173}{\text{SCE}_2 = `r format(round(sse2b, 1), nsmall = 0L, big.mark = ",")`}}{\color{#314f4f}{\text{SCE}_1 = `r format(round(sse1b, 1), nsmall = 0L, big.mark = ",")`}} \approx `r format(round(sse2b/sse1b, 1), nsmall = 0L, big.mark = ",")` \implies$ *p*-valor $\approx `r round(pf(sse2b/sse1b, 375, 375, lower.tail = F), 3)`$

$\therefore$ Fallamos en rechazar la hip√≥tesis H.sub[0]: $\sigma^2_1 = \sigma^2_2$ cuando hay heterocedasticidad.
---
layout: true
# Testeando la heterocedasticidad
## El test de Breusch-Pagan 
---

Breusch y Pagan (1981) intentaron resolver este problema de ser muy espec√≠ficos con la forma funcional de la heterocedasticidad.

- Permite a los datos mostrar c√≥mo la varianza de $u_i$ se correlaciona con $X$.

- Si $\sigma_i^2$ se correlaciona con $X$, entonces tenemos heterocedasticidad.

- Regresa $e_i^2$ en $X = \left[ 1,\, x_1,\, x_2,\, \ldots,\, x_k \right]$ y testea la significancia conjunta.
---

C√≥mo se implementa:

.pseudocode-small[

1\. Regresa y en el intercepto, x.sub[1], x.sub[2], ‚Ä¶, x.sub[k].

2\. Se queda con los residuos e.

3\. Regresa e.super[2] en el intercepto, x.sub[1], x.sub[2], ‚Ä¶, x.sub[k].

$$e\_i^2 = \alpha\_0 + \alpha\_1 x\_{1i} + \alpha\_2 x\_{2i} + \cdots + \alpha\_k x\_{ki} + v\_i$$

4\. Se queda con R.super[2].

5\. Testea la hip√≥tesis H.sub[0]: $\alpha_1 = \alpha_2 = \cdots = \alpha_k = 0$

]

---

El estad√≠stico del test de B-P es

$$\text{LM} = n \times R^2_{e}$$

donde $R^2_e$ es el $R^2$ de la regresi√≥n

$$e\_i^2 = \alpha\_0 + \alpha\_1 x\_{1i} + \alpha\_2 x\_{2i} + \cdots + \alpha\_k x\_{ki} + v\_i$$

Bajo la nula, $\text{LM}$ se distribuye asint√≥ticamente $\chi^2_k$.

--

Este estad√≠stico testeat H.sub[0]: $\alpha_1 = \alpha_2 = \cdots = \alpha_k = 0$.

Rechazar la hipotesis nula implica que hay evidencia de heterocedasticidad.

---
layout: true
# Testeando la heterocedasticidad
## La distribuci√≥n $\chi^2$

---

Acabamos de mencionar que bajo el valor nulo, el estad√≠stico de la prueba B-P se distribuye como una variable aleatoria $\chi^2$ con $k$ grados de libertad.

La distribuci√≥n $\chi ^ 2$ es solo otro ejemplo de una distribuci√≥n com√∫n (con nombre) (como la distribuci√≥n Normal, la distribuci√≥n $t$ y la $F$).

---

Tres ejemplos de $\chi_k^2$: $\color{#314f4f}{k = 1}$, $\color{#e64173}{k = 2}$, y $\color{orange}{k = 9}$

```{R, chisq1, echo = F, dev = "svg", fig.height = 5.5}
ggplot(data = tibble(x = c(0, 20)), aes(x)) +
  geom_area(
    stat = "function", fun = dchisq, args = list(df = 2),
    fill = "darkslategrey", alpha = 0.3
  ) +
  stat_function(
    fun = dchisq, args = list(df = 2), n = 1e3,
    color = "darkslategrey"
  ) +
  geom_area(
    stat = "function", fun = dchisq, args = list(df = 3),
    fill = red_pink, alpha = 0.3
  ) +
  stat_function(
    fun = dchisq, args = list(df = 3), n = 1e3,
    color = red_pink
  ) +
  geom_area(
    stat = "function", fun = dchisq, args = list(df = 9),
    fill = "orange", alpha = 0.3
  ) +
  stat_function(
    fun = dchisq, args = list(df = 9), n = 1e3,
    color = "orange"
  ) +
  labs(x = "x", y = "f") +
  theme_axes_math
```
---

Probabilidad de observar un estad√≠stico m√°s extremo $\widehat{\text{LM}}$ bajo H.sub[0]

```{R, chisq2, echo = F, dev = "svg", fig.height = 5.5}
ggplot(data = tibble(x = c(0, 8)), aes(x)) +
  geom_area(
    stat = "function", fun = dchisq, args = list(df = 2),
    fill = "darkslategrey", alpha = 0.05
  ) +
  geom_area(
    stat = "function", fun = dchisq, args = list(df = 2),
    fill = red_pink, alpha = 0.85,
    xlim = c(5, 8)
  ) +
  stat_function(
    fun = dchisq, args = list(df = 2), n = 1e3,
    color = "darkslategrey"
  ) +
  geom_vline(xintercept = 5, color = grey_dark, size = 0.5, linetype = "dotted") +
  annotate("text", x = 5, y = 1.55 * dchisq(5, df = 2), label = TeX("$\\widehat{LM}$"), size = 7) +
  labs(x = "x", y = "f") +
  theme_axes_math
```
---
layout: true
# Testeando la heterocedasticidad
## El test de Breusch-Pagan
---

**Problema:** Seguimos asumiendo una .hi[forma funcional] bastante restrictiva entre nuestras variables explicativas $X$ y las variaciones de nuestras perturbaciones $\sigma ^ 2_i$.

--

**Resultado:** B-P *puede* a√∫n pasar por alto formas bastante simples de heterocedasticidad.

---

Las pruebas de Breusch-Pagan siguen siendo .hi[sensibles a la forma funcional].

```{R, bp1, echo = F, dev = "svg", fig.height = 3.75}
set.seed(12345)
# Data
bp_df <- tibble(
  x = runif(1e3, -3, 3),
  e = rnorm(1e3, 0, sd = 2 + x^2),
  y = 1 + 3 * x + e
)
# Regressions
lm_bp1 <- lm(residuals(lm(y ~ x, bp_df))^2 ~ 1 + bp_df$x) %>%
  summary() %$% r.squared %>% multiply_by(1e3)
lm_bp2 <- lm(residuals(lm(y ~ x, bp_df))^2 ~ 1 + bp_df$x + I(bp_df$x^2)) %>%
  summary() %$% r.squared %>% multiply_by(1e3)
# The figure
ggplot(data = bp_df, aes(x = x, y = e)) +
geom_point(size = 2.75, color = "darkslategrey", alpha = 0.5) +
labs(x = "x", y = "u") +
theme_axes_math
```

$$
\begin{aligned}
  e_i^2 &= \hat{\alpha}_0 + \hat{\alpha}_1 x_{1i} & \widehat{\text{LM}} &= `r round(lm_bp1, 2)` &\mathit{p}\text{-value} \approx `r round(pchisq(lm_bp1, 1, lower.tail = F), 3)` \\
  e_i^2 &= \hat{\alpha}_0 + \hat{\alpha}_1 x_{1i} \color{#e64173}{+ \hat{\alpha}_2 x^2_{1i}} & \widehat{\text{LM}} &= `r round(lm_bp2, 2)` &\mathit{p}\text{-value} < 0.001
\end{aligned}
$$
---
layout: true
# Testeando la heterocedasticidad
## El test de White
---

Hasta ahora hemos estado probando relaciones espec√≠ficas entre nuestras variables explicativas y las varianzas de las perturbaciones, por ejemplo,

- H.sub[0]: $\sigma_1^2 = \sigma_2^2$ para dos grupos basados en $x_j$ (**G-Q**)

- H.sub[0]: $\alpha_1 = \cdots = \alpha_k = 0$ de $e_i^2 = \alpha_0 + \alpha_1 x_{1i} + \cdots + \alpha_k x_{ki} + v_i$ (**B-P**)

--

Sin embargo, en realidad queremos saber si

$$\sigma_1^2 = \sigma_2^2 = \cdots = \sigma_n^2$$

**P:** ¬øNo podemos simplemente probar esta hip√≥tesis?

--

**R:** M√°s o menos.
---

Con este objetivo, Hal White aprovech√≥ el hecho de que podemos .hi[reemplazar el requisito de homocedasticidad con una suposici√≥n m√°s d√©bil]:

- **Viejo:** $\mathop{\text{Var}} \left( u_i \middle| X \right) = \sigma^2$

- **Nuevo:** $u^2$ est√° *incorrelacionado* con las variables explicativas (_por ejemplo_,  $x_j$ para todo $j$), sus cuadrados (_por ejemplo_, $x_j^2$), y las interacciones d eprimer grado (_por ejemplo_, $x_j x_h$).

--

Este nuevo supuesto es m√°s f√°cil de probar expl√≠citamente (*pista:* regresi√≥n).
---

Un resumen de la prueba de White para heterocedasticidad:

.pseudocode-small[

1\. Regresamos y en x.sub[1], x.sub[2], ‚Ä¶, x.sub[k]. Guardo los residuos e.

2\. los residuos al cuadrado de todas las variables explicativas, sus cuadrados e interacciones.

3\. Guardo R.sub[e].super[2].

4\. Calculo el estad√≠stico del test para testear H.sub[0]: $\alpha_p = 0$ para todo $p\neq0$.

]
---

Al igual que con la prueba de Breusch-Pagan, la estad√≠stica de prueba de White es

$$\text{LM} = n \times R_e^2 \qquad \text{Bajo H}_0,\, \text{LM} \overset{\text{d}}{\sim} \chi_k^2$$

pero ahora $R^2_e$ proviene de la regresi√≥n de $e^2$ sobre las variables explicativas, sus cuadrados y sus interacciones.


**Nota:** El $k$ (para nuestro $\chi_k^2$) es igual al n√∫mero de par√°metros estimados en la regresi√≥n de arriba 
(el $\alpha_j$), excluyendo el intercepto $\left( \alpha_0 \right)$.
---

**Nota pr√°ctica:** Si una variable es igual a su cuadrado (_ejemplo_, variables binarias), entonces no puede incluirla. La misma regla se aplica a las interacciones.
---

*Ejemplo:* Considere el modelo.super[‚Ä†] $y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \beta_3 x_3 + u$

**Paso 1:** Estime el modelo; obtenga los residuos $(e)$.

**Paso 2:** Regrese $e^2$ en las variables explicativas, sus cuadrados e interacciones.
$$
\begin{aligned}
  e^2 = \
  & \alpha_0 + \alpha_1 x_1 + \alpha_2 x_2 + \alpha_3 x_3 + \alpha_4 x_1^2 + \alpha_5 x_2^2 + \alpha_6 x_3^2 \\
  &+ \alpha_7 x_1 x_2 + \alpha_8 x_1 x_3 + \alpha_9 x_2 x_3 + v
\end{aligned}
$$

Guarde el R.super[2] de esta ecuaci√≥n (llam√©moslo, $R_e^2$).

**Paso 3:** Testeo H.sub[0]: $\alpha_1 = \alpha_2 = \cdots = \alpha_9 = 0$ using $\text{LM} = n R^2_e \overset{\text{d}}{\sim} \chi_9^2$.

.footnote[
[‚Ä†]: Para simplificar la notaci√≥n, no utilizo los sub√≠ndices $i$.
]
---

```{R, white1, echo = F, dev = "svg", fig.height = 4}
set.seed(12345)
# The figure
ggplot(data = bp_df, aes(x = x, y = e)) +
geom_point(size = 2.75, color = "darkslategrey", alpha = 0.5) +
labs(x = "x", y = "u") +
theme_axes_math
```

Ya hicimos la prueba de White para esta regresi√≥n lineal simple.

$$
\begin{aligned}
 e_i^2 &= \hat{\alpha}_0 + \hat{\alpha}_1 x_{1i} \color{#e64173}{+ \hat{\alpha}_2 x^2_{1i}} & \widehat{\text{LM}} &= `r round(lm_bp2, 2)` &\mathit{p}\text{-value} < 0.001
\end{aligned}
$$
---
layout: true
# Heterocedasticidad
## Preguntas de repaso
---

--

- **P:** ¬øCu√°l es la definici√≥n de heterocedastidad?

- **P:** ¬øPor qu√© nos preocupa la heterocedastidad?

- **P:** ¬øGraficar $y$ contra $x$, nos dice algo sobre la heterocedastidad?

- **P:** ¬øGraficar $e$ contra $x$, nos dice algo sobre la heterocedastidad?

- **P:** Dado que no podemos observar $u_i$'s, ¬øqu√© podemos usar para *aprender m√°s* sobre la heterocedasticidad?

- **P:** ¬øQu√© test se recomiendo para la heterocedasticidad? ¬øPor qu√©?

---
count: false

- **P:** ¬øCu√°l es la definici√≥n de heterocedastidad?
--

- **R:**
<br>.hi[Matem√°tica:] $\mathop{\text{Var}} \left( u_i | X \right) \neq \mathop{\text{Var}} \left( u_j | X \right)$ para alg√∫n $i\neq j$.
<br>.hi[Palabras:] Hay una relaci√≥n sistem√°tica entre la varianza de $u_i$ y nuestras variables explicativas.
---
count: false

.grey-vlight[

- **P:** ¬øCu√°l es la definici√≥n de heterocedastidad?

]

- **P:** ¬øPor qu√© nos preocupa la heterocedastidad?
--

- **R:** Esto sesga nuestros errores est√°ndar, arruinando nuestras pruebas estad√≠sticas e intervalos de confianza. Adem√°s: MCO ya no es el MELI.
---
count: false

.grey-vlight[

- **P:** ¬øCu√°l es la definici√≥n de heterocedastidad?

- **P:** ¬øPor qu√© nos preocupa la heterocedastidad?

]

- **P:** ¬øGraficar $y$ contra $x$, nos dice algo sobre la heterocedastidad?
--

- **R:** No es exactamente lo que queremos, pero dado que $y$ es una funci√≥n de $x$ y $u$, a√∫n puede ser informativo. Si $y$ se vuelve m√°s/menos disperso a medida que cambia $x$, es probable que tengamos heterocedasticidad.
---
count: false

.grey-vlight[

- **P:** ¬øCu√°l es la definici√≥n de heterocedastidad?

- **P:** ¬øPor qu√© nos preocupa la heterocedastidad?

- **P:** ¬øGraficar $y$ contra $x$, nos dice algo sobre la heterocedastidad?

]

- **P:** ¬øGraficar $e$ contra $x$, nos dice algo sobre la heterocedastidad?
--

- **R:** Si. El margen de $e$ representa su varianza y nos dice algo sobre la varianza de $u$. Las tendencias en esta varianza, a lo largo de $x$, sugieren heterocedasticidad.
---
count: false

.grey-vlight[

- **P:** ¬øCu√°l es la definici√≥n de heterocedastidad?

- **P:** ¬øPor qu√© nos preocupa la heterocedastidad?

- **P:** ¬øGraficar $y$ contra $x$, nos dice algo sobre la heterocedastidad?

- **P:** ¬øGraficar $e$ contra $x$, nos dice algo sobre la heterocedastidad?

]

- **P:** Dado que no podemos observar $u_i$'s, ¬øqu√© podemos usar para *aprender m√°s* sobre la heterocedasticidad?
--

- **R:** Usamos los $e_i$ para predecir/aprender sobre los $u_i$. Este truco es clave para casi todo lo que hacemos con la prueba/correcci√≥n de heterocedasticidad.
---
count: false

.grey-vlight[

- **P:** ¬øCu√°l es la definici√≥n de heterocedastidad?

- **P:** ¬øPor qu√© nos preocupa la heterocedastidad?

- **P:** ¬øGraficar $y$ contra $x$, nos dice algo sobre la heterocedastidad?

- **P:** ¬øGraficar $e$ contra $x$, nos dice algo sobre la heterocedastidad?

- **P:** Dado que no podemos observar $u_i$'s, ¬øqu√© podemos usar para *aprender m√°s* sobre la heterocedasticidad?

]


---
layout: true
# Heterocedasticidad - ejercicio

---
class: inverse, middle
---
¬øEn qu√© subfiguras de abajo probablemente $u_i$ es heteroced√°stico? Expliquen brevemente. 
(***Ayuda*** Puede haber m√°s de una.)

**Figura 1**
```{R, echo = F, dev = "svg", fig.height = 2.5}
set.seed(123)
n <- 101
# No violations
p1 <- ggplot(data = tibble(x = 1:n, u = rnorm(n)), aes(x = x, y = u)) +
geom_hline(yintercept = 0) +
scale_y_continuous(breaks = 0) +
geom_point() +
ggtitle("Figura 1a") +
theme_axes
# Violates homoskedasticity
p2 <- ggplot(data = tibble(x = 1:n, u = rnorm(n, sd = abs(sin(x/(100))) + 0.1)), aes(x = x, y = u)) +
geom_hline(yintercept = 0) +
scale_y_continuous(breaks = 0) +
geom_point() +
ggtitle("Figura 1b") +
theme_axes
# Violates both
p3 <- ggplot(data = tibble(x = 1:n, u = runif(n, min = -250, max = (x-50.5)^2)), aes(x = x, y = u)) +
geom_hline(yintercept = 0) +
scale_y_continuous(breaks = 0) +
geom_point() +
ggtitle("Figura 1c") +
theme_axes
# Put it all together
grid.arrange(p1, p2, p3, nrow = 1)
```

---


.pink[
**Respuesta:** $u_i$ es probablemente heteroced√°stico en las subfiguras **1b** y **1c**. Podemos ver tendencias claras (relaciones) entre la varianza de $u_i$ (su dispersi√≥n) y $x_i$.
]

---
**1b.** En la presencia de heterocedasticidad, ¬øMCO sigue siendo insesgado?

--
.pink[
**Respuesta:** S√≠.
]


**1c.** ¬øQu√© problemas causa la heterocedasticidad en nuestro setting de MCO?
--

.pink[
**Respuesta** La heterocedasticidad hace a (1) MCO ineficiente y (2) sesga la estimaci√≥n de los errores est√°ndar.
]

**1d.** Imaginemos que queremos estimar por MCO este modelo

$$
\begin{align}
  y_i = \beta_0 + \beta_1 x_i + u_i \tag{1}
\end{align}
$$

donde $x_i$ es una variable categ√≥rica que toma los valores $1$, $2$, o $3$.

Supongamos que sabemos que $\mathop{\text{Var}} \left( u_i \middle| x_i = 1 \right) = 15$ y $\mathop{\text{Var}} \left( u_i \middle| x_i = 2 \right) = 15$. No conocemos $\mathop{\text{Var}} \left( u_i \middle| x_i = 3 \right)$, _i.e._, $\mathop{\text{Var}} \left( u_i | x_i = 3 \right) = \sigma_3^2$ para alg√∫n par√°metro desconocido $\sigma_3^2$.

¬øQu√© valor deber√≠a tomar $\sigma_3^2$ para que nuestro modelo sea homoced√°stico?

--

.pink[
**Respuesta** Para que nuestro modelo sea homoced√°stico, $\sigma_3^2 = 15$.
]
---

**1e.** *Goldfeld-Quandt* Para probar si los datos que usaremos para estimar la ecuaci√≥n $(1)$ son homoced√°sticos / heterosced√°sticos, haremos una prueba de Goldfeld-Quandt. 

Estimamos $(1)$ para el tercio superior del conjunto de datos (ordenados en $x$) y encontramos SSE.sub [3] = 100. Estimamos $(1)$ en el tercio medio y encontramos SSE.sub [2] = 80. Finalmente, estimamos $(1)$ en el tercio inferior y encontramos SSE.sub [1] = 70. Cada uno de estos tres grupos tiene 100 observaciones. Realice una prueba de Goldfeld-Quandt. Exprese sus hip√≥tesis, calcule el estad√≠stico de la prueba G-Q, determine el valor *p*, concluya.

***Sugerencia:*** La funci√≥n `pf(q, df1, df2, lower.tail = F)` calcula la probabilidad de observar un valor de `q` o uno mayor en una distribuci√≥n $F$ con `df1, df2` grados de libertad numerador y denominador.

--

.pink[
**Respuesta** La hipotesis para nuestro test es

.b[H.sub[o]]: $\sigma_1^2 = \sigma_3^2$ (homocedasticidad) *vs.* .b[H.sub[a]]: $\sigma_1^2 \neq \sigma_3^2$ (heterocedasticidad)

Para la prueba de Goldfeld-Quandt, probamos esta hip√≥tesis nula utilizando el estad√≠stico de prueba

$$
\begin{align}
   F = \dfrac{SSE_3}{SSE_1} = \dfrac{100}{70} \approx 1.4286
\end{align}
$$

Bajo la hip√≥tesis nula, este estad√≠stico de prueba tiene una distribuci√≥n $F$ con 98 (= 100-2) grados de libertad en el numerador y denominador. Usando .mono[R] podemos calcular el *p*-valor:

```{R, key-2f}
# p-valor
pf(100/70, df1 = 100-2, df2 = 100-2, lower.tail = F)
```

Este valor *p* es menor que 0.05, por lo que rechazamos la hip√≥tesis nula y concluimos que hay evidencia estad√≠sticamente significativa de heterocedasticidad (al nivel del 5 por ciento).
]
---
layout: true
# Viviendo con heterocedasticidad
---
class: inverse, middle, true
---

E¬°n la presencia de heterocedasticidad, MCO es

- todav√≠a .hi[insesgado]
- pero... .hi[ya no es m√°s eficiente]

En promedio, obtenemos la respuesta correcta pero con m√°s ruido (menos precisi√≥n).
<br> *Adem√°s:* Nuestros errores est√°ndar est√°n sesgados.

--

**Opciones:**

1. Compruebe la .hi[especificaci√≥n] de la regresi√≥n.
2. Encuentre un nuevo y m√°s eficiente .hi[estimador insesgado] para $\beta_j$'s.
3. Vivir con la ineficiencia de MCO; encontrar un .hi[nuevo estimador de varianza].
  - Errores est√°ndar
  - Intervalos de confianza
  - Pruebas de hip√≥tesis
---
layout: true
# Viviendo con heterocedasticidad
## Error de especificaci√≥n
---

Como hemos comentado, la especificaci√≥n .pink[<sup> ‚Ä† </sup>] del modelo de regresi√≥n es muy importante para la insesgadez y la eficiencia de su estimador.

**Respuesta \#1:** Aseg√∫rense de que su especificaci√≥n no cause heterocedasticidad.

.footnote[.pink[‚Ä†] *Especificaci√≥n:* Forma funcional y variables incluidas.]
---

*Ejemplo:* Dejemos que la relacion poblacional sea $$y_i = \beta_0 + \beta_1 x_i + \beta_2 x_i^2 + u_i$$

con $\mathop{\boldsymbol{E}}\left[ u_i \middle| x_i \right] = 0$ y $\mathop{\text{Var}} \left( u_i \middle| x_i \right) = \sigma^2$.

Sin embargo, omitimos $x^2$ y estimamos $$y_i = \gamma_0 + \gamma_1 x_i + w_i$$

Entonces $$w_i = u_i + \beta_2 x_i^2 \implies \mathop{\text{Var}} \left( w_i \right) = f(x_i)$$

_Es decir_, la varianza de $w_i$ cambia sistem√°ticamente con $x_i$ (heterocedasticidad).
---

```{R, spec data, include = F}
# Set the seed
set.seed(1234)
# Generate data
spec_df <- tibble(x = runif(1e3, 0, 3), y = exp(0.5 + 0.6 * x + rnorm(1e3, sd = 0.3)))
# Add residuals ('w': wrong specification; 'c': correct specification)
spec_df %<>% dplyr::mutate(
  e_w = lm(y ~ x, data = spec_df) %>% residuals(),
  e_c = lm(log(y) ~ x, data = spec_df) %>% residuals()
)
```

.pink[Verdad:] $\color{#e64173}{\log\left(y_i\right) = \beta_0 + \beta_1 x_i + u_i}$ ‚ÄÉ.slate[**Error de especificaci√≥n:**] $\color{#314f4f}{y_i = \beta_0 + \beta_1 x_i + v_i}$

```{R, spec plot1, echo = F, dev = "svg", fig.height = 5}
ggplot(data = spec_df, aes(x = x)) +
  geom_point(aes(y = e_w), color = "darkslategrey", size = 2.75, alpha = 0.5, shape = 16) +
  geom_point(aes(y = e_c), color = red_pink, size = 2.5, alpha = 0, shape = 19) +
  labs(x = "x", y = "e") +
  theme_axes_math
```
---

.pink[**Verdad:**] $\color{#e64173}{\log\left(y_i\right) = \beta_0 + \beta_1 x_i + u_i}$ ‚ÄÉ.slate[Error de especificaci√≥n:] $\color{#314f4f}{y_i = \beta_0 + \beta_1 x_i + v_i}$

```{R, spec plot2, echo = F, dev = "svg", fig.height = 5}
ggplot(data = spec_df, aes(x = x)) +
  geom_point(aes(y = e_w), color = "darkslategrey", size = 2.75, alpha = 0.25, shape = 1) +
  geom_point(aes(y = e_c), color = red_pink, size = 2.5, alpha = 0.5, shape = 19) +
  labs(x = "x", y = "e") +
  theme_axes_math
```
---

De manera m√°s general:

**El problema del error de especificaci√≥n:** La incorrecta especificaci√≥n del modelo de regresi√≥n puede causar (entre otros problemas).

--

** Soluci√≥n: ** üí° Lo hacemos bien bien (_ejemplo_, no omiten $x^2$).

--

**Nuevos problemas:**

- A menudo no conocemos la especificaci√≥n *correcta*.
- Nos gustar√≠a un proceso m√°s formal para abordar la heterocedasticidad.

--

**Conclusi√≥n:** La especificaci√≥n a menudo no "resolver√°" la heterocedasticidad.
<br> Sin embargo, especificar correctamente su modelo sigue siendo realmente importante.

---
layout: true
# Viviendo con heterocedasticidad
## M√≠nimos Cuadrados Ponderados
---

M√≠nimos Cuadrados Ponderados (MCP) presentan otro enfoque.

**Respuesta \#2:** Aumenta la eficiencia ponderando nuestras observaciones.

--

Sea la verdadera relaci√≥n de poblaci√≥n

$$
\begin{align}
  y_i = \beta_0 + \beta_1 x_{i} + u_i \tag{1}
\end{align}
$$

con $u_i \sim \mathop{N} \left( 0,\, \sigma_i^2 \right)$.

--

Ahora transformamos $(1)$ dividiendo cada observaci√≥n por $\sigma_i$, _es decir_,

$$
\begin{align}
  \dfrac{y_i}{\sigma_i} &= \beta_0 \dfrac{1}{\sigma_i} + \beta_1 \dfrac{x_{i}}{\sigma_i} + \dfrac{u_i}{\sigma_i} \tag{2}
\end{align}
$$

---

$$
\begin{align}
  y_i &= \beta_0 + \beta_1 x_{i} + u_i \tag{1} \\[1em]
  \dfrac{y_i}{\sigma_i} &= \beta_0 \dfrac{1}{\sigma_i} + \beta_1 \dfrac{x_{i}}{\sigma_i} + \dfrac{u_i}{\sigma_i} \tag{2}
\end{align}
$$

Mientras que $(1)$ sea heteroced√°stico,
--
 $\color{#e64173}{(2)}$ .hi[es homoced√°stico].

‚à¥ MCO es eficiente e insesgado para estimar los $\beta_k$ en $(2)$!

--

¬øPor qu√© $(2)$ es homoced√°stico?

--

$\mathop{\text{Var}} \left( \dfrac{u_i}{\sigma_i} \middle| x_i \right) =$
--
 $\dfrac{1}{\sigma_i^2} \mathop{\text{Var}} \left( u_i \middle| x_i \right) =$
--
 $\dfrac{1}{\sigma_i^2} \sigma_i^2 =$
--
 $1$
---

MCP es genial, pero necesitamos saber $\sigma_i^2$, que generalmente es poco probable.

Podemos relajar *ligeramente* este requisito, en lugar de requerir

1. $\mathop{\text{Var}} \left(u_i | x_i \right) = \sigma_i^2 = \sigma^2h(x_i)$

2. Conocemos $h(x)$.

--

Como antes, transformamos nuestro modelo heteroced√°stico en un modelo homoced√°stico. Esta vez dividimos los datos de cada observaci√≥n <sup>.pink[‚Ä†]</sup> por $\sqrt{h(x_i)}$.

.note[
.pink[‚Ä†] Divide *todos* los datos por $\sqrt{h(x_i)}$, incluida la constante.
]
---

$$
\begin{align}
  y_i &= \beta_0 + \beta_1 x_{i} + u_i \tag{1} \\[1em]
  \dfrac{y_i}{\sqrt{h(x_i)}} &= \beta_0 \dfrac{1}{\sqrt{h(x_i)}} + \beta_1 \dfrac{x_{i}}{\sqrt{h(x_i)}} + \dfrac{u_i}{\sqrt{h(x_i)}} \tag{2}
\end{align}
$$
con $\mathop{\text{Var}} \left( u_i | x_i \right) = \sigma^2 h(x_i)$.

--

Ahora chequeemos que $(2)$ sea efectivamente homoced√°stico.

$\mathop{\text{Var}} \left( \dfrac{u_i}{\sqrt{h(x_i)}} \middle| x_i \right) =$
--
 $\dfrac{1}{h(x_i)} \mathop{\text{Var}} \left( u_i \middle| x_i \right) =$
--
 $\dfrac{1}{h(x_i)} \sigma^2 h(x_i) =$
--
 $\color{#e64173}{\sigma^2}$

.hi[¬°Homocedasticidad!]
---

Los estimadores por .hi[M√≠nimos Cuadrados Ponderados] (MCP) son una clase especial de los estimadores estimators .hi[M√≠nimos Cuadrados Generalizados] (MCG) pero enfocados en heterocedasticidad.

--

$$
  y\_i = \beta\_0 + \beta\_1 x\_{1i} + u\_i \quad \color{#e64173}{\text{ vs. }} \quad
  \dfrac{y\_i}{\sigma\_i} = \beta\_0 \dfrac{1}{\sigma\_i} + \beta\_1 \dfrac{x\_{1i}}{\sigma\_i} + \dfrac{u\_i}{\sigma\_i}
$$

*Notas:*

1. MCP **transforma** un modelo heteroced√°stico en un modelo homoced√°stico.
2. **Ponderando:** MCP le da peso menor a las observaciones con mayor varianza de $u_i$'s.
3. **Gran requisito:** MCP requiere que *sepamos* $\sigma_i^2$ para cada observaci√≥n.
4. MCP es generalmente **inviable**. *Factible* MCGF ofrecen una soluci√≥n.
5. Bajo sus supuestos: MCP es el **mejor estimador lineal insesgado**.
---
layout: true
# Viviendo con heterocedasticidad
## Errores est√°ndar robustos-heteroced√°sticos
---

**Respuesta \#3:**

- Ignoramos la ineficiencia de MCO (en presencia de heterocedasticidad).
- Nos centramos en .hi[estimaciones no sesgadas de nuestros errores est√°ndar].
- En el proceso: Inferencia correcta üòé

--

**P:** ¬øQu√© es un error est√°ndar?
--

<br>**R:**  La .hi[desviaci√≥n est√°ndar de la distribuci√≥n de un estimador].

--

Los estimadores (como $\hat{\beta}_1$) son variables aleatorias, por lo que tienen distribuciones.

Los errores est√°ndar nos dan una idea de cu√°nta variabilidad hay en nuestro estimador.

---

*Recuerden*: podemos escribir el estimador de MCO para $\beta_1$ como

$$\hat{\beta}_1 = \beta\_1 + \dfrac{\sum_i \left( x_i - \overline{x} \right) u_i}{\sum_i \left( x_i - \overline{x} \right)^2} = \beta_1 + \dfrac{\sum_i \left( x_i - \overline{x} \right) u_i}{\text{SCT}_x} \tag{3}$$

--

Sea $\mathop{\text{Var}} \left( u_i \middle| x_i \right) = \sigma_i^2$.

--

Podemos usar $(3)$ para ecsribir la varianza de $\hat{\beta}_1$, _es decir_,

$$\mathop{\text{Var}} \left( \hat{\beta}_1 \middle| x_i \right) = \dfrac{\sum_i \left( x_i - \overline{x} \right)^2 \sigma_i^2}{\text{SCT}_x^2} \tag{4}$$
---

Si queremos estimaciones insesgadas para nuestros errores est√°ndar, necesitamos una estimaci√≥n no sesgada para

$$\dfrac{\sum_i \left( x_i - \overline{x} \right)^2 \sigma_i^2}{\text{SCT}_x^2}$$

Nuestro viejo amigo (?) Hal White nos dio tal estimador:.pink[<sup>‚Ä†</sup>]

$$\widehat{\mathop{\text{Var}}} \left( \hat{\beta}_1 \right) = \dfrac{\sum_i \left( x_i - \overline{x} \right)^2 e_i^2}{\text{SCT}_x^2}$$

donde $e_i$ viene de la regresi√≥n MCO de inter√©s.

.footnote[
.pink[‚Ä†] Esta ecuaci√≥n espec√≠fica es para regresi√≥n lineal simple.
]
---

Nuestra estimaci√≥n de errores est√°ndar robustos-heteroced√°sticos para $\beta_j$.

.hi[Caso 1] Regresi√≥n lineal simple, $y_i = \beta_0 + \beta_1 x_i + u_i$

$$\widehat{\mathop{\text{Var}}} \left( \hat{\beta}_1 \right) = \dfrac{\sum_i \left( x_i - \overline{x} \right)^2 e_i^2}{\text{SCT}_x^2}$$

.hi[Caso 2] Regresi√≥n lineal m√∫ltiple, $y_i = \beta_0 + \beta_1 x_{1i} + \cdots + \beta_k x_{ki} + u_i$

$$\widehat{\mathop{\text{Var}}} \left( \hat{\beta}_j \right) = \dfrac{\sum_i \hat{r}_{ij}^2 e_i^2}{\text{SCT}_{x_j^2}}$$

donde $\hat{r}_{ij}$ denota el i.super[√©simo] residual producto de regresar $x_j$ contra las dem√°s variables explicativas.
---

Con estos errores est√°ndar, podemos volver hacer inferencia correctamente.

_Ejemplo_, podemos actualizar nuestra f√≥rmula del estad√≠stico $t$ con nuestro nuevo error est√°ndar robusto a la heterocedasticidad.

$$t = \frac{\text{Estimaci√≥n Puntual - Valor Hipotetizado}}{\text{Error est√°ndar}}$$
---
layout: false
class: inverse, middle

# Viviendo con heterocedasticidad
## Ejemplos
---
layout: true
# Viviendo con heterocedasticidad
---

## Ejemplos

De nuevo al set de datos de puntajes en pruebas...

```{R, ex test data}
# Load packages
library(Ecdat)
library(tidyverse)
# Select y rename desired variables; assign to new dataset; format as tibble
test_df <- Caschool %>% select(
  test_score = testscr, ratio = str, income = avginc, enrollment = enrltot
) %>% as_tibble()
# View first 2 rows of the dataset
head(test_df, 2)
```
---
layout: true
# Viviendo con heterocedasticidad
## Ejemplo: Error de especificaci√≥n
---

Encontramos evidencia significativa de heterocedasticidad.

Comprobemos si se debi√≥ a una especificaci√≥n incorrecta de nuestro modelo.
---

Modelo.sub[1]: $\text{Puntaje}_i = \beta_0 + \beta_1 \text{Ratio}_i + \beta_2 \text{Ingreso}_i + u_i$
<br>`lm(test_score ~ ratio + income, data = test_df)`


```{R, ex spec1, echo = F, dev = "svg", fig.height = 4.75}
# Modelo 1: test ~ ratio + income
test_df %<>% mutate(e1 = lm(test_score ~ ratio + income, data = test_df) %>% residuals())
# Plot
ggplot(data = test_df, aes(x = income, y = e1)) +
geom_point(size = 3, alpha = 0.5, color = red_pink) +
labs(x = "Ingreso", y = TeX("\\textit{e}")) +
theme_axes_serif
```
---

Modelo.sub[2]: $\log\left(\text{Puntaje}_i\right) = \beta_0 + \beta_1 \text{Ratio}_i + \beta_2 \text{Ingreso}_i + u_i$
<br>`lm(log(test_score) ~ ratio + income, data = test_df)`


```{R, ex spec2, echo = F, dev = "svg", fig.height = 4.75}
# Modelo 1: test ~ ratio + income
test_df %<>% mutate(e2 = lm(log(test_score) ~ ratio + income, data = test_df) %>% residuals())
# Plot
ggplot(data = test_df, aes(x = income)) +
geom_point(aes(y = e2), size = 3, alpha = 0.5, color = red_pink) +
labs(x = "Ingreso", y = TeX("\\textit{e}")) +
theme_axes_serif
```
---

Modelo.sub[3]: $\log\left(\text{Puntaje}_i\right) = \beta_0 + \beta_1 \text{Ratio}_i + \beta_2 \log\left(\text{Ingreso}_i\right) + u_i$
<br>`lm(log(test_score) ~ ratio + log(income), data = test_df)`


```{R, ex spec3, echo = F, dev = "svg", fig.height = 4.75}
# Modelo 1: test ~ ratio + income
test_df %<>% mutate(e3 = lm(log(test_score) ~ ratio + log(income), data = test_df) %>% residuals())
# Plot
ggplot(data = test_df, aes(x = income)) +
geom_point(aes(y = e3), size = 3, alpha = 0.5, color = red_pink) +
labs(x = "Ingreso", y = TeX("\\textit{e}")) +
theme_axes_serif
```
---

Testeamos esta nueva especificaci√≥n con el test de White:

.center[
Modelo.sub[3]: $\log\left(\text{Puntaje}_i\right) = \beta_0 + \beta_1 \text{Ratio}_i + \beta_2 \log\left(\text{Ingreso}_i\right) + u_i$
]

```{R, ex spec test, include = F}
white_r2_spec <- lm(e3^2 ~
  ratio * log(income) + I(ratio^2) + I(log(income)^2),
  data = test_df
) %>% summary() %$% r.squared
white_stat_spec <- white_r2_spec %>% multiply_by(420)
```

--
La regresi√≥n para el test de White 

--
$$\begin{align}
  e_i^2 = &\alpha_0 + \alpha_1 \text{Ratio}_i + \alpha_2 \log\left(\text{Ingreso}_i\right) + \alpha_3 \text{Ratio}_i^2 + \alpha_4 \left(\log\left(\text{Ingreso}_i\right)\right)^2 \\
  &+ \alpha_5 \left(\text{Ratio}_i\times\log\left(\text{Ingreso}_i\right)\right) + v_i
\end{align}$$
--
con un $R_e^2\approx`r round(white_r2_spec, 3)`$
--
 y el estad√≠stico t
--
 $\widehat{\text{LM}} = n\times R_e^2 \approx `r round(white_stat_spec, 1)`$.

--

Bajo H.sub[0], $\text{LM}$ se distribuye
--
 $\chi_5^2$
--
 $\implies$ *p*-valor $\approx$ `r pchisq(white_stat_spec, 5, lower.tail = F) %>% round(3)`.

--

‚à¥
--
 .hi[Rechazamos H.sub[0].]

--
 .hi[Conclusi√≥n:]

--

Existe evidencia estad√≠sticamente significativa de heterocedasticidad al nivel del cinco por ciento.

---

De acuerdo, intentamos ajustar nuestra especificaci√≥n, pero todav√≠a hay evidencia de heterocedasticidad.

**Siguiente:** En general, vamos a ir por los errores est√°ndar robustos.

- MCO sigue siendo insesgado para los .hi[coeficientes] (los $\beta_j$'s)

- los errores est√°ndar robustos son insesgados para los .hi[errores est√°ndar] de los $\hat{\beta}_j$'s, _es decir_, $\sqrt{\mathop{\text{Var}} \left( \hat{\beta}_j \right)}$.
---
layout: true
# Viviendo con heterocedasticidad
## Ejemplos: Errores est√°ndar robustos
---

Volvemos a nuestro modelo

$$\text{Puntaje}_i = \beta_0 + \beta_1 \text{Ratio}_i + \beta_2 \text{Ingreso}_i + u_i$$

Podemos usar el paquete `lfe` en .mono[R] para calcular los errores est√°ndar.
---

$$ \text{Puntaje}_i = \beta_0 + \beta_1 \text{Ratio}_i + \beta_2 \text{Ingreso}_i + u_i $$

1\. Corremos la regresi√≥n con `felm()` (en vez de `lm()`)
```{R, lfe1}
# Load 'lfe' package
library(lfe)
# Regress log score on ratio y log income
test_reg <- felm(test_score ~ ratio + income, data = test_df)
```

--

*(!)* `felm()` usa la misma sint√°xis de regresi√≥n que `lm()`.
---

$$\text{Puntaje}_i = \beta_0 + \beta_1 \text{Ratio}_i + \beta_2 \text{Ingreso}_i + u_i$$

2\. Estimamos los errores est√°ndar robustos con la opci√≥n `robust = T` en `summary()`
```{R, lfe2a, eval = F}
# Het-robust standard errors con 'robust = T'
summary(test_reg, robust = T)
```
```{R, lfe2b, echo = F}
test_het_out <- summary(test_reg, robust = T) %>% capture.output()
test_het_out[10:13] %>% paste0("\n") %>% cat()
```
---

Coeficientes y **errores est√°ndar robustos**:
```{R, lfe3, eval = F}
summary(test_reg, robust = T)
```
```{R, lfe4, echo = F}
test_het_out <- summary(test_reg, robust = T) %>% capture.output()
test_het_out[10:13] %>% paste0("\n") %>% cat()
```

Coeficiencientes y **errores est√°ndar MCO** (asumiendo homocedasticidad):
```{R, lfe5, eval = F}
summary(test_reg, robust = F)
```
```{R, lfe6, echo = F}
test_hom_out <- summary(test_reg, robust = F) %>% capture.output()
test_hom_out[10:13] %>% paste0("\n") %>% cat()
```
---
layout: true
# Viviendo con heterocedasticidad
## Ejemplo: MCP
---

Mencionamos que MCP no es posible muchas veces porque debemos saber la forma funcional de heterocedasticidad

**A**\. $\sigma_i^2$

o

**B**\. $h(x_i)$, donde $\sigma_i^2 = \sigma^2 h(x_i)$

--

*Hay* ocasiones en que podemos conocer $h(x_i)$.
---

Imag√≠nense que los individuos de una poblaci√≥n tienen perturbaciones homoced√°sticass.

Sin embargo, en lugar de observar los datos de las personas, observamos (en los datos) los promedios de los grupos (por ejemplo, ciudades, condados, distritos escolares).

Si estos grupos tienen diferentes tama√±os, entonces nuestro conjunto de datos ser√° heteroced√°stico, de una manera predecible.

**Recuerden:** La varianza de la media muestral depende del tama√±o de la muestra,

$$\mathop{\text{Var}} \left( \overline{x} \right) = \dfrac{\sigma_x^2}{n}$$

--

**Ejemplo:**  Los datos de las pruebas de nuestra escuela se promedian a nivel escolar.
---

*Ejemplo:*  Los datos de las pruebas de nuestra escuela se promedian a nivel escolar.

Incluso si los estudiantes tuvieran perturbaciones homoced√°sticas, las escuelas podr√≠an tener perturbaciones heteroced√°sticas, _por ejemplo_,

**Modelo a nivel individual:** $\quad \text{Puntaje}_i = \beta_0 + \beta_1 \text{Ratio}_i + \beta_2 \text{Ingreso}_i + u_i$

**Modelo a nivel escuela:** $\quad \overline{\text{Puntaje}}_s = \beta_0 + \beta_1 \overline{\text{Ratio}}_s + \beta_2 \overline{\text{Ingreso}}_s + \overline{u}_s$

donde el sub√≠ndice $s$ denota el una escuela.

$$\mathop{\text{Var}} \left( \overline{u}_s \right) = \dfrac{\sigma^2}{n_s}$$
---

Para MCP, buscamos una funci√≥n $h(x_s)$ tal que $\mathop{\text{Var}} \left( \overline{u}_s | x_s \right) = \sigma^2 h(x_s)$.

--

Acabamos de mostrar<sup>.pink[‚Ä†]</sup> que $\mathop{\text{Var}} \left( \overline{u}_s |x_s \right) = \dfrac{\sigma^2}{n_s}$.

.footnote[
.pink[‚Ä†] Asumiendo que las perturbaciones individuales son homoced√°sticos.
]

--

Entonces, $h(x_s) = 1/n_s$, donde $n_s$ es e√± n√∫mero de estudiantes en la escuela $s$.

--

Para implementar MCP, dividimos los datos de cada observaci√≥n por $1/\sqrt{h(x_s)}$, lo que significa que necesitamos multiplicar los datos de cada escuela por $\sqrt{n_s}$.

--

La variable .mono[enrollment] (inscripci√≥n) en el conjunto de datos .mono[test_df] es nuestro $n_s$.

---

Usando MCP para estimar $\text{Puntaje}_i = \beta_0 + \beta_1 \text{Ratio}_i + \beta_2 \text{Ingreso}_i + u_i$

**Paso 1:** Multiplica cada variable por $1/ \sqrt{h(x_i)} = \sqrt{\text{Enrollment}_i}$

```{R, wls1}
# Create MCP transformed variables, multiplying by sqrt of 'pop'
test_df <- mutate(test_df,
  test_score_wls = test_score * sqrt(enrollment),
  ratio_wls      = ratio * sqrt(enrollment),
  income_wls     = income * sqrt(enrollment),
  intercept_wls  = 1 * sqrt(enrollment)
)
```

Observe que estamos creando una intersecci√≥n transformada.
---

Usando MCP para estimar $\text{Puntaje}_i = \beta_0 + \beta_1 \text{Ratio}_i + \beta_2 \text{Ingreso}_i + u_i$

**Paso 2:** Corremos nuestra regresi√≥n de MCP transformados

```{R, wls2}
# MCP regression
wls_reg <- lm(
  test_score_wls ~ -1 + intercept_wls + ratio_wls + income_wls,
  data = test_df
)
```
--
*Nota:* El `-1` en nuestra regresi√≥n le dice a .mono[R] que no agregue una intersecci√≥n, ya que estamos agregando una intersecci√≥n transformada (`intercept_wls`).
---

Las .hi[estimaciones y sus errores est√°ndar de MCP:]
```{R, wls3, echo = F}
# Grab the summary
test_wls_out <- summary(wls_reg) %>% capture.output()
# Print the coefficients
test_wls_out[11:14] %>% paste0("\n") %>% cat()
```
--
<br>
Las .hi[estimaciones MCO] y .hi[los errores robustos a la hetorocedasticidad:]
```{R, wls4, echo = F}
# Print the coefficients
test_het_out[10:13] %>% paste0("\n") %>% cat()
```

---
layout: true
# Autocorrelaci√≥n

---
class: inverse, middle

---
# Autocorrelaci√≥n
## ¬øQu√© es?

La .hi[autocorrelaci√≥n] ocurre cuando nuestras perturbaciones est√°n correlacionadas en el tiempo, _es decir_, $\mathop{\text{Cov}} \left( u_t,\, u_s \right) \neq 0$ para $t\neq s$.

--

Otra forma de pensar: si el *shock* de la perturbaci√≥n $t$ se correlaciona con los shocks "cercanos" en $t-1$ y $t + 1$.

--

*Nota:* La **correlaci√≥n serial** y la **autocorrelaci√≥n** son lo mismo.

---
layout: false
# Autocorrelaci√≥n
## MCO

Para **modelos est√°ticos** o **modelos din√°micos con variables explicativas rezagadas**, en presencia de autocorrelaci√≥n:

1. MCO proporciona estimaciones .pink[**insesgadas** para los coeficientes].

2. MCO crea .pink[estimaciones **sesgadas** para los errores est√°ndar].

3. MCO es .pink[**ineficiente**.]

*Recuerden:* Las mismas implicancias que la heterocedasticidad.

