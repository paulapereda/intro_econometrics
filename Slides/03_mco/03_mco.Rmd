---
title: "MCO: primeros pasos"
subtitle: "Econometr铆a I"
author: "Paula Pereda (ppereda@correo.um.edu.uy)"
date: "27 de agosto de 2021"
output:
  xaringan::moon_reader:
    css: ['default', 'metropolis', 'metropolis-fonts', 'my-css.css']
    # self_contained: true
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
---
class: inverse, middle

```{r Setup, include = F}
options(htmltools.dir.version = FALSE)

knitr::knit_engines$set(upper = function(options) {
  code <- paste(options$code, collapse = "\n")
  if (options$eval) 
    toupper(code) else code
})

library(pacman)
p_load(broom, latex2exp, ggplot2, ggthemes, viridis, dplyr, magrittr, knitr, parallel)
# Define pink color
red_pink <- "#e64173"
# Notes directory
# Knitr options
opts_chunk$set(
  comment = ">",
  fig.align = "center",
  fig.height = 7,
  fig.width = 10.5,
  # dpi = 300,
  warning = F,
  message = F
)
# A blank theme for ggplot
theme_empty <- theme_bw() + theme(
  line = element_blank(),
  rect = element_blank(),
  strip.text = element_blank(),
  axis.text = element_blank(),
  plot.title = element_blank(),
  axis.title = element_blank(),
  plot.margin = structure(c(0, 0, -1, -1), unit = "lines", valid.unit = 3L, class = "unit"),
  legend.position = "none"
)
theme_simple <- theme_bw() + theme(
  line = element_blank(),
  panel.grid = element_blank(),
  rect = element_blank(),
  strip.text = element_blank(),
  axis.text.x = element_text(size = 14),
  axis.text.y = element_blank(),
  axis.ticks = element_blank(),
  plot.title = element_blank(),
  axis.title = element_blank(),
  # plot.margin = structure(c(0, 0, -1, -1), unit = "lines", valid.unit = 3L, class = "unit"),
  legend.position = "none"
)
theme_axes <- theme_empty + theme(
  axis.title = element_text(size = 18),
  plot.margin = structure(c(0, 0, 0.1, 0), unit = "lines", valid.unit = 3L, class = "unit"),
)
```


```{R, colors, include = F}
# Define pink color
red_pink <- "#e64173"
turquoise <- "#20B2AA"
orange <- "#FFA500"
red <- "#fb6107"
blue <- "#2b59c3"
green <- "#8bb174"
grey_light <- "grey70"
grey_mid <- "grey50"
grey_dark <- "grey20"
purple <- "#6A5ACD"
```

layout: false
class: inverse, middle
# Repaso

---
layout: true
# Poblaci贸n *vs.* muestra

---

## Modelos y notaci贸n

Escribimos nuestro modelo poblacional (simple) as铆:

$$ y_i = \beta_0 + \beta_1 x_i + u_i $$

y nuestro modelo de regresi贸n estimado basado en la muestra as铆:

$$ y_i = \hat{\beta}_0 + \hat{\beta}_1 x_i + e_i $$

Un modelo estimado de regresi贸n produce estimaciones para cada observaci贸n:

$$ \hat{y}_i = \hat{\beta}_0 + \hat{\beta}_1 x_i $$

lo que nos da la l铆nea que _mejor se ajusta_ a nuestra muestra.

---
layout: true

# Poblaci贸n *vs.* muestra

**Pregunta:** 驴Por qu茅 nos importa la *poblaci贸n vs. muestra*?

---

--

```{R, gen dataset, include = F, cache = T}
# Set population y sample sizes
n_p <- 100
n_s <- 30
# Set the seed
set.seed(1989)
# Generate data
pop_df <- tibble(
  i = 3,
  x = rnorm(n_p, mean = 5, sd = 1.5),
  e = rnorm(n_p, mean = 0, sd = 1),
  y = i + 0.5 * x + e,
  row = rep(1:sqrt(n_p), times = sqrt(n_p)),
  col = rep(1:sqrt(n_p), each = sqrt(n_p)),
  s1 = sample(x = c(rep(T, n_s), rep(F, n_p - n_s))),
  s2 = sample(x = c(rep(T, n_s), rep(F, n_p - n_s))),
  s3 = sample(x = c(rep(T, n_s), rep(F, n_p - n_s)))
)
# Regressions
lm0 <- lm(y ~ x, data = pop_df)
lm1 <- lm(y ~ x, data = filter(pop_df, s1 == T))
lm2 <- lm(y ~ x, data = filter(pop_df, s2 == T))
lm3 <- lm(y ~ x, data = filter(pop_df, s3 == T))
# Simulation
set.seed(1989)
sim_df <- mclapply(mc.cores = 10, X = 1:1e4, FUN = function(x, size = n_s) {
  lm(y ~ x, data = pop_df %>% sample_n(size = size)) %>% tidy()
}) %>% do.call(rbind, .) %>% as_tibble()
```

.pull-left[

```{R, pop1, echo = F, fig.fullwidth = T, dev = "svg"}
ggplot(data = pop_df, aes(x = row, y = col)) +
geom_point(color = "darkslategray", size = 10) +
theme_empty
```

.center[**Poblaci贸n**]

]

--

.pull-right[

```{R, scatter1, echo = F, fig.fullwidth = T, dev = "svg"}
ggplot(data = pop_df, aes(x = x, y = y)) +
geom_abline(
  intercept = lm0$coefficients[1], slope = lm0$coefficients[2],
  color = red_pink, size = 3
) +
geom_point(color = "darkslategray", size = 6) +
theme_empty
```

.center[**Relaci贸n poblacional**]

$$ y_i = `r round(lm0$coefficients[1], 2)` + `r round(lm0$coefficients[2], 2)` x_i + u_i $$

$$ y_i = \beta_0 + \beta_1 x_i + u_i $$


]

---

.pull-left[

```{R, sample1, echo = F, fig.fullwidth = T, dev = "svg"}
ggplot(data = pop_df, aes(x = row, y = col, shape = s1)) +
geom_point(color = "darkslategray", size = 10) +
scale_shape_manual(values = c(1, 19)) +
theme_empty
```

.center[**Muestra 1:** 30 individuos aleatorios]

]

--

.pull-right[

```{R, sample1 scatter, echo = F, fig.fullwidth = T, dev = "svg"}
ggplot(data = pop_df, aes(x = x, y = y)) +
geom_abline(
  intercept = lm0$coefficients[1], slope = lm0$coefficients[2],
  color = red_pink, size = 3, alpha = 0.3
) +
geom_point(aes(shape = s1), color = "darkslategray", size = 6) +
geom_abline(
  intercept = lm1$coefficients[1], slope = lm1$coefficients[2],
  size = 2, linetype = 2, color = "black"
) +
scale_shape_manual(values = c(1, 19)) +
theme_empty
```

.center[

**Relaci贸n poblacional**
<br>
$y_i = `r round(lm0$coefficients[1], 2)` + `r round(lm0$coefficients[2], 2)` x_i + u_i$

**Relaci贸n muestral**
<br>
$\hat{y}_i = `r round(lm1$coefficients[1], 2)` + `r round(lm1$coefficients[2], 2)` x_i$

]

]

---
count: false

.pull-left[

```{R, sample2, echo = F, fig.fullwidth = T, dev = "svg"}
ggplot(data = pop_df, aes(x = row, y = col, shape = s2)) +
geom_point(color = "darkslategray", size = 10) +
scale_shape_manual(values = c(1, 19)) +
theme_empty
```

.center[**Muestra 2:** 30 individuos aleatorios]

]

.pull-right[

```{R, sample2 scatter, echo = F, fig.fullwidth = T, dev = "svg"}
ggplot(data = pop_df, aes(x = x, y = y)) +
geom_abline(
  intercept = lm0$coefficients[1], slope = lm0$coefficients[2],
  color = red_pink, size = 3, alpha = 0.3
) +
geom_point(aes(shape = s2), color = "darkslategray", size = 6) +
geom_abline(
  intercept = lm1$coefficients[1], slope = lm1$coefficients[2],
  size = 2, linetype = 2, color = "black", alpha = 0.3
) +
geom_abline(
  intercept = lm2$coefficients[1], slope = lm2$coefficients[2],
  size = 2, linetype = 2, color = "black"
) +
scale_shape_manual(values = c(1, 19)) +
theme_empty
```

.center[

**Relaci贸n poblacional**
<br>
$y_i = `r round(lm0$coefficients[1], 2)` + `r round(lm0$coefficients[2], 2)` x_i + u_i$

**Relaci贸n muestral**
<br>
$\hat{y}_i = `r round(lm2$coefficients[1], 2)` + `r round(lm2$coefficients[2], 2)` x_i$

]

]
---
count: false

.pull-left[

```{R, sample3, echo = F, fig.fullwidth = T, dev = "svg"}
ggplot(data = pop_df, aes(x = row, y = col, shape = s3)) +
geom_point(color = "darkslategray", size = 10) +
scale_shape_manual(values = c(1, 19)) +
theme_empty
```

.center[**Muestra 3:** 30 individuos aleatorios]

]

.pull-right[

```{R, sample3 scatter, echo = F, fig.fullwidth = T, dev = "svg"}
ggplot(data = pop_df, aes(x = x, y = y)) +
geom_abline(
  intercept = lm0$coefficients[1], slope = lm0$coefficients[2],
  color = red_pink, size = 3, alpha = 0.3
) +
geom_point(aes(shape = s3), color = "darkslategray", size = 6) +
geom_abline(
  intercept = lm1$coefficients[1], slope = lm1$coefficients[2],
  size = 2, linetype = 2, color = "black", alpha = 0.3
) +
geom_abline(
  intercept = lm2$coefficients[1], slope = lm2$coefficients[2],
  size = 2, linetype = 2, color = "black", alpha = 0.3
) +
geom_abline(
  intercept = lm3$coefficients[1], slope = lm3$coefficients[2],
  size = 2, linetype = 2, color = "black"
) +
scale_shape_manual(values = c(1, 19)) +
theme_empty
```

.center[

**Relaci贸n poblacional**
<br>
$y_i = `r round(lm0$coefficients[1], 2)` + `r round(lm0$coefficients[2], 2)` x_i + u_i$

**Relaci贸n muestral**
<br>
$\hat{y}_i = `r round(lm3$coefficients[1], 2)` + `r round(lm3$coefficients[2], 2)` x_i$

]

]

---
layout: false
class: clear, middle

Vamos a repetir esto **10.000 veces**.

(Este ejercicio se llama simulaci贸n Monte Carlo.)

---
layout: true
# Poblaci贸n *vs.* muestra

---

```{R, simulation scatter, echo = F, dev = "png", dpi = 300, cache = T}
# Reshape sim_df
line_df <- tibble(
  intercept = sim_df %>% filter(term != "x") %>% select(estimate) %>% unlist(),
  slope = sim_df %>% filter(term == "x") %>% select(estimate) %>% unlist()
)
ggplot() +
geom_abline(data = line_df, aes(intercept = intercept, slope = slope), alpha = 0.01) +
geom_point(data = pop_df, aes(x = x, y = y), size = 3, color = "darkslategray") +
geom_abline(
  intercept = lm0$coefficients[1], slope = lm0$coefficients[2],
  color = red_pink, size = 1.5
) +
theme_empty
```

---
layout: true
# Poblaci贸n *vs.* muestra

---

.pull-left[
```{R, simulation scatter2, echo = F, dev = "png", dpi = 300, cache = T}
# Reshape sim_df
line_df <- tibble(
  intercept = sim_df %>% filter(term != "x") %>% select(estimate) %>% unlist(),
  slope = sim_df %>% filter(term == "x") %>% select(estimate) %>% unlist()
)
ggplot() +
geom_abline(data = line_df, aes(intercept = intercept, slope = slope), alpha = 0.01, size = 1) +
geom_point(data = pop_df, aes(x = x, y = y), size = 6, color = "darkslategray") +
geom_abline(
  intercept = lm0$coefficients[1], slope = lm0$coefficients[2],
  color = red_pink, size = 3
) +
theme_empty
```
]

.pull-right[

- En **promedio**, nuestras l铆neas de regresi贸n coinciden muy bien con la l铆nea poblacional.

- Sin embargo, **l铆neas individuales** (muestras) realmente pueden fallar.

- Las diferencias entre las muestras individuales y la poblaci贸n generan **incertidumbre** para el o la econometrista.
]

---

**Pregunta:** 驴Por qu茅 nos importa la *poblaci贸n vs. muestra*?

--

**Respuesta:** La incertidumbre importa.

$\hat{\beta}$ en s铆 mismo es una variable aleatoria, que depende de la muestra aleatoria. Cuando tomamos una muestra y corremos una regresi贸n, no sabemos si es una 'buena' muestra, $\hat{\beta}$ est谩 cerca de $\beta$, o una 'mala muestra', nuestra muestra difiere mucho de la poblaci贸n.

---
layout: false
# Poblaci贸n *vs.* muestra

## Incertidumbre

Hacer un seguimiento de esta incertidumbre ser谩 clave en el curso.

- Estimaci贸n de errores est谩ndar para nuestras estimaciones.

- Prueba de hip贸tesis.

- Correcci贸n de heterocedasticidad y autocorrelaci贸n.

--

Primero, repasemos c贸mo obtenemos estas estimaciones de regresi贸n (inciertas).

---
# Regresi贸n lineal

## El estimador

Podemos estimar la l铆nea de regresi贸n en .mono[R] (`lm(y ~ x, my_data)`) pero... 驴de d贸nde vienen estas estimaciones? 


Unas diapositivas atr谩s:

> $$ \hat{y}_i = \hat{\beta}_0 + \hat{\beta}_1 x_i $$
> lo que nos da la l铆nea que *mejor se ajusta* a nuestra muestra (set de datos).

驴Pero a qu茅 nos referimos con la "l铆nea que mejor se ajusta"?

---
layout: false

# Siendo el "mejor"

**Pregunta:** 驴A qu茅 nos referimos con la *l铆nea que mejor se ajusta*?

**Respuestas:**

- En general (en econometr铆a), la *l铆nea que mejor se ajusta* significa la l铆nea que minimiza la suma de minimizando la suma de errores o residuos al cuadrado (SCR):

.center[

$\text{SCR} = \sum_{i = 1}^{n} e_i^2\quad$ donde $\quad e_i = y_i - \hat{y}_i$

]

- **M铆nimos Cuadrados** Ordinarios (**MCO**) minimiza la suma de errores o residuos al cuadrado.
- Basado en un conjunto de supuestos (en su mayor铆a aceptables), MCO
  - Es insesgado (y consistente)
  - Es el *mejor* (m铆nima varianza) estimador lineal insesgado (MELI)

---
layout: true
# MCO *vs.* otras l铆neas/estimadores

---

Consideramos el set de datos generado anteriormente.

```{R, ols vs lines 1, echo = F, dev = "svg", fig.height = 6}
ggplot(data = pop_df, aes(x = x, y = y)) +
geom_point(size = 5, color = "darkslategray", alpha = 0.9) +
theme_empty
```

---
count: false

Para cualquier l铆nea $\left(\hat{y} = \hat{\beta}_0 + \hat{\beta}_1 x\right)$

```{R, vs lines 2, echo = F, dev = "svg", fig.height = 6}
# Define a function
y_hat <- function(x, b0, b1) {b0 + b1 * x}
# Define line's parameters
b0 <- 6
b1 <- 0.2
# The plot
ggplot(data = pop_df, aes(x = x, y = y)) +
# geom_segment(aes(x = x, xend = x, y = y, yend = y_hat(x, b0, b1)), size = 0.5, alpha = 0.2) +
geom_point(size = 5, color = "darkslategray", alpha = 0.9) +
geom_abline(intercept = b0, slope = b1, color = "orange", size = 2, alpha = 0.9) +
theme_empty
```

---
count: false

Para cualquier l铆nea $\left(\hat{y} = \hat{\beta}_0 + \hat{\beta}_1 x\right)$, podemos calcular los errores: $e_i = y_i - \hat{y}_i$

```{R, ols vs lines 3, echo = F, dev = "svg", fig.height = 6}
# Define a function
y_hat <- function(x, b0, b1) {b0 + b1 * x}
# Define line's parameters
b0 <- 6
b1 <- 0.2
# The plot
ggplot(data = pop_df, aes(x = x, y = y)) +
geom_segment(aes(x = x, xend = x, y = y, yend = y_hat(x, b0, b1)), size = 0.5, alpha = 0.2) +
geom_point(size = 5, color = "darkslategray", alpha = 0.9) +
geom_abline(intercept = b0, slope = b1, color = "orange", size = 2, alpha = 0.9) +
theme_empty
```

---
count: false

Para cualquier l铆nea $\left(\hat{y} = \hat{\beta}_0 + \hat{\beta}_1 x\right)$, podemos calcular los errores: $e_i = y_i - \hat{y}_i$

```{R, ols vs lines 4, echo = F, dev = "svg", fig.height = 6}
# Define a function
y_hat <- function(x, b0, b1) {b0 + b1 * x}
# Define line's parameters
b0 <- 3
b1 <- 0.2
# The plot
ggplot(data = pop_df, aes(x = x, y = y)) +
geom_segment(aes(x = x, xend = x, y = y, yend = y_hat(x, b0, b1)), size = 0.5, alpha = 0.2) +
geom_point(size = 5, color = "darkslategray", alpha = 0.9) +
geom_abline(intercept = b0, slope = b1, color = "orange", size = 2, alpha = 0.9) +
theme_empty
```

---
count: false

Para cualquier l铆nea $\left(\hat{y} = \hat{\beta}_0 + \hat{\beta}_1 x\right)$, podemos calcular los errores: $e_i = y_i - \hat{y}_i$

```{R, ols vs lines 5, echo = F, dev = "svg", fig.height = 6}
# Define a function
y_hat <- function(x, b0, b1) {b0 + b1 * x}
# Define line's parameters
b0 <- 10
b1 <- -0.8
# The plot
ggplot(data = pop_df, aes(x = x, y = y)) +
geom_segment(aes(x = x, xend = x, y = y, yend = y_hat(x, b0, b1)), size = 0.5, alpha = 0.2) +
geom_point(size = 5, color = "darkslategray", alpha = 0.9) +
geom_abline(intercept = b0, slope = b1, color = "orange", size = 2, alpha = 0.9) +
theme_empty
```

---
count: false

La SCR expresa los errores al cuadrado $\left(\sum e_i^2\right)$: mayores errores, reciben mayores penalizaciones.

```{R, ols vs lines 6, echo = F, dev = "svg", fig.height = 6}
# Define a function
y_hat <- function(x, b0, b1) {b0 + b1 * x}
# Define line's parameters
b0 <- 10
b1 <- -0.8
# The plot
ggplot(data = pop_df, aes(x = x, y = y)) +
geom_segment(aes(x = x, xend = x, y = y, yend = y_hat(x, b0, b1), color = (y - y_hat(x, b0, b1))^2), size = 0.5, alpha = 0.8) +
geom_point(size = 5, color = "darkslategray", alpha = 0.9) +
geom_abline(intercept = b0, slope = b1, color = "orange", size = 2, alpha = 0.9) +
scale_color_viridis(option = "cividis", direction = -1) +
theme_empty
```

---
count: false

El estimador de MCO es una combinaci贸n de $\hat{\beta}_0$ y $\hat{\beta}_1$ que minimizan la SCR.

```{R, ols vs lines 7, echo = F, dev = "svg", fig.height = 6}
# Define a function
y_hat <- function(x, b0, b1) {b0 + b1 * x}
# Define line's parameters
b0 <- lm0$coefficients[1]
b1 <- lm0$coefficients[2]
# The plot
ggplot(data = pop_df, aes(x = x, y = y)) +
geom_segment(aes(x = x, xend = x, y = y, yend = y_hat(x, b0, b1), color = (y - y_hat(x, b0, b1))^2), size = 0.5, alpha = 0.8) +
geom_point(size = 5, color = "darkslategray", alpha = 0.9) +
geom_abline(intercept = b0, slope = b1, color = red_pink, size = 2, alpha = 0.9) +
scale_color_viridis(option = "cividis", direction = -1) +
theme_empty
```

---
layout: true
# MCO

## Formalmente

---

En una regresi贸n lineal simple, el estimador de MCO proviene de escoger los $\hat{\beta}_0$ y $\hat{\beta}_1$ que minimizan la suma de errores o residuos al cuadrado (SCR), _es decir_,

$$\min_{\hat{\beta}_0,\, \hat{\beta}_1} \text{SCR}$$

--

pero ya sabemos que $\text{SCR} = \sum_i e_i^2$, ahora usamos las definiciones de $e_i$ y $\hat{y}$.

$$
\begin{aligned}
  e_i^2 &= \left( y_i - \hat{y}_i \right)^2 = \left( y_i - \hat{\beta}_0 - \hat{\beta}_1 x_i \right)^2 \\
  &= y_i^2 - 2 y_i \hat{\beta}_0 - 2 y_i \hat{\beta}_1 x_i + \hat{\beta}_0^2 + 2 \hat{\beta}_0 \hat{\beta}_1 x_i + \hat{\beta}_1^2 x_i^2
\end{aligned}
$$

--

**Recuerden:** Minimizar una funci贸n multivariante requiere (**1**) primeras derivadas iguales a cero (las *condiciones de primer orden*) y (**2**) condiciones de segundo orden (concavidad).

---

Nos estamos acercando. Necesitamos **minimizar las SCR**. Hemos mostrado c贸mo SCR se relaciona con nuestra muestra (nuestros datos: $x$ y $y$) y nuestras estimaciones, _es decir_, $\hat{\beta}_0$ y $\hat{\beta}_1$.

$$ \text{SCR} = \sum_i e_i^2 = \sum_i \left( y_i^2 - 2 y_i \hat{\beta}_0 - 2 y_i \hat{\beta}_1 x_i + \hat{\beta}_0^2 + 2 \hat{\beta}_0 \hat{\beta}_1 x_i + \hat{\beta}_1^2 x_i^2 \right) $$

Para las condiciones de primer orden de minimizaci贸n, tomamos la primera derivada de la SCR con respecto $\hat{\beta}_0$ y $\hat{\beta}_1$.

$$
\begin{aligned}
  \dfrac{\partial \text{SCR}}{\partial \hat{\beta}_0} &= \sum_i \left( 2 \hat{\beta}_0 + 2 \hat{\beta}_1 x_i - 2 y_i \right) = 2n \hat{\beta}_0 + 2 \hat{\beta}_1 \sum_i x_i - 2 \sum_i y_i \\
  &= 2n \hat{\beta}_0 + 2n \hat{\beta}_1 \overline{x} - 2n \overline{y}
\end{aligned}
$$

donde $\overline{x} = \frac{\sum x_i}{n}$ y $\overline{y} = \frac{\sum y_i}{n}$ son medias muestrales de $x$ y $y$ (tama帽o $n$).

---

Las condiciones de primer orden establecen que las derivadas son iguales a cero, entonces:

$$ \dfrac{\partial \text{SCR}}{\partial \hat{\beta}_0} = 2n \hat{\beta}_0 + 2n \hat{\beta}_1 \overline{x} - 2n \overline{y} = 0 $$

lo que implica que

$$ \hat{\beta}_0 = \overline{y} - \hat{\beta}_1 \overline{x} $$

Ahora para $\hat{\beta}_1$.

---

Tomamos la derivada de SCR con respecto a $\hat{\beta}_1$

$$
\begin{aligned}
  \dfrac{\partial \text{SCR}}{\partial \hat{\beta}_1} &= \sum_i \left( 2 \hat{\beta}_0 x_i + 2 \hat{\beta}_1 x_i^2 - 2 y_i x_i \right) = 2 \hat{\beta}_0 \sum_i x_i + 2 \hat{\beta}_1 \sum_i x_i^2 - 2 \sum_i y_i x_i \\
  &= 2n \hat{\beta}_0 \overline{x} + 2 \hat{\beta}_1 \sum_i x_i^2 - 2 \sum_i y_i x_i
\end{aligned}
$$

lo igualamos a cero (condiciones de primer orden, de nuevo )

$$ \dfrac{\partial \text{SCR}}{\partial \hat{\beta}_1} = 2n \hat{\beta}_0 \overline{x} + 2 \hat{\beta}_1 \sum_i x_i^2 - 2 \sum_i y_i x_i = 0 $$

y sustituimos en nuestra relaci贸n por $\hat{\beta}_0$, _es decir_, $\hat{\beta}_0 = \overline{y} - \hat{\beta}_1 \overline{x}$. Entonces,

$$
 2n \left(\overline{y} - \hat{\beta}_1 \overline{x}\right) \overline{x} + 2 \hat{\beta}_1 \sum_i x_i^2 - 2 \sum_i y_i x_i = 0
$$

---

Continuando de la diapositiva anterior

$$ 2n \left(\overline{y} - \hat{\beta}_1 \overline{x}\right) \overline{x} + 2 \hat{\beta}_1 \sum_i x_i^2 - 2 \sum_i y_i x_i = 0 $$

multiplicamos

$$ 2n \overline{y}\,\overline{x} - 2n \hat{\beta}_1 \overline{x}^2 + 2 \hat{\beta}_1 \sum_i x_i^2 - 2 \sum_i y_i x_i = 0 $$

$$ \implies 2 \hat{\beta}_1 \left( \sum_i x_i^2 - n \overline{x}^2 \right) = 2 \sum_i y_i x_i - 2n \overline{y}\,\overline{x} $$

$$ \implies \hat{\beta}_1 = \dfrac{\sum_i y_i x_i - 2n \overline{y}\,\overline{x}}{\sum_i x_i^2 - n \overline{x}^2} = \dfrac{\sum_i (x_i - \overline{x})(y_i - \overline{y})}{\sum_i (x_i - \overline{x})^2} $$

---

隆Listo! 

Ahora tenemos adorables estimadores de MCO para la pendiente

$$ \hat{\beta}_1 = \dfrac{\sum_i (x_i - \overline{x})(y_i - \overline{y})}{\sum_i (x_i - \overline{x})^2} $$

y el intercepto

$$ \hat{\beta}_0 = \overline{y} - \hat{\beta}_1 \overline{x} $$

Ahora sabemos de donde viene la parte de *m铆nimos cuadrados* de M铆nimos Cuadrados Ordinarios 

--

Pasamos ahora a los supuestos y propiedades (impl铆citas) de MCO.

---
layout: false
class: inverse, middle

# MCO: Supuestos y propiedades

---
layout: true
# MCO: Supuestos y propiedades

## Propiedades
---

**Pregunta:** 驴Qu茅 propiedades nos podr铆an interesar para un estimador?

--

**Tangente:** Revisemos primero las propiedades estad铆sticas.

---

**Refrescando:** Funciones de densidad

Recuerden que usamos **funciones de densidad de probabilidad** para describir la probabilidad que una **variable aleatoria continua** adopte un rango de valores. (El 谩rea total = 1)

Estas caracterizan distribuciones de probabilidad, y las distribuciones m谩s comunes/famosas/populares obtienen nombres (_por ejemplo_, normal, *t*, Gamma).

---

**Refrescando:** Funciones de densidad

La probabilidad de que una variable aleatoria normal est谩ndar tome un valor entre -2 y 0: $\mathop{\text{P}}\left(-2 \leq X \leq 0\right) = 0.48$

```{R, example: pdf, echo = F, dev = "svg", fig.height = 3.5}
# Generate data for density's polygon
tmp <- tibble(x = seq(-4, 4, 0.01), y = dnorm(x))
tmp <- rbind(tmp, tibble(x = seq(4, -4, -0.01), y = 0))
# Plot it
ggplot(data = tmp, aes(x, y)) +
geom_polygon(fill = "grey85") +
geom_polygon(data = tmp %>% filter(between(x, -2, 0)), fill = red_pink) +
geom_hline(yintercept = 0, color = "black") +
theme_simple
```

---

**Refrescando:** Funciones de densidad

La probabilidad de que una variable aleatoria normal est谩ndar tome un valor entre -1.96 y 1.96: $\mathop{\text{P}}\left(-1.96 \leq X \leq 1.96\right) = 0.95$

```{R, example: pdf 2, echo = F, dev = "svg", fig.height = 3.5}
# Generate data for density's polygon
tmp <- tibble(x = seq(-4, 4, 0.01), y = dnorm(x))
tmp <- rbind(tmp, tibble(x = seq(4, -4, -0.01), y = 0))
# Plot it
ggplot(data = tmp, aes(x, y)) +
geom_polygon(fill = "grey85") +
geom_polygon(data = tmp %>% filter(between(x, -1.96, 1.96)), fill = red_pink) +
geom_hline(yintercept = 0, color = "black") +
theme_simple
```

---

**Refrescando:** Funciones de densidad

La probabilidad de que una variable aleatoria normal est谩ndar adquiera un valor superior a 2: $\mathop{\text{P}}\left(X > 2\right) = 0.023$

```{R, example: pdf 3, echo = F, dev = "svg", fig.height = 3.5}
# Generate data for density's polygon
tmp <- tibble(x = seq(-4, 4, 0.01), y = dnorm(x))
tmp <- rbind(tmp, tibble(x = seq(4, -4, -0.01), y = 0))
# Plot it
ggplot(data = tmp, aes(x, y)) +
geom_polygon(fill = "grey85") +
geom_polygon(data = tmp %>% filter(between(x, 2, Inf)), fill = red_pink) +
geom_hline(yintercept = 0, color = "black") +
theme_simple
```

---

Imagine que estamos tratando de estimar un par谩metro desconocido $\beta$, y conocemos las distribuciones de tres estimadores en competencia. 驴Cu谩l querr铆amos? 驴C贸mo decidir铆amos?

```{R, competing pdfs, echo = F, dev = "svg", fig.height = 4.5}
# Generate data for densities' polygons
d1 <- tibble(x = seq(-7.5, 7.5, 0.01), y = dnorm(x, mean = 1, sd = 1)) %>%
  rbind(., tibble(x = seq(7.5, -7.5, -0.01), y = 0))
d2 <- tibble(x = seq(-7.5, 7.5, 0.01), y = dunif(x, min = -2.5, max = 1.5)) %>%
  rbind(., tibble(x = seq(7.5, -7.5, -0.01), y = 0))
d3 <- tibble(x = seq(-7.5, 7.5, 0.01), y = dnorm(x, mean = 0, sd = 2.5)) %>%
  rbind(., tibble(x = seq(7.5, -7.5, -0.01), y = 0))
# Plot them
ggplot() +
geom_polygon(data = d1, aes(x, y), alpha = 0.8, fill = "orange") +
geom_polygon(data = d2, aes(x, y), alpha = 0.65, fill = red_pink) +
geom_polygon(data = d3, aes(x, y), alpha = 0.6, fill = "darkslategray") +
geom_hline(yintercept = 0, color = "black") +
geom_vline(xintercept = 0, size = 1, linetype = "dashed") +
scale_x_continuous(breaks = 0, labels = TeX("$\\beta$")) +
theme_simple +
theme(axis.text.x = element_text(size = 20))
```

---

**Pregunta:** 驴Qu茅 propiedades nos podr铆an interesar para un estimador?

--

**Respuesta uno: Sesgo.**

En promedio (despu茅s de *muchas* muestras), 驴el estimador tiende hacia el valor correcto?

**M谩s formalmente:** 驴La media de la distribuci贸n del estimador es igual al par谩metro que se estima?

$$ \mathop{\text{Sesgo}}_\beta \left( \hat{\beta} \right) = \mathop{\boldsymbol{E}}\left[ \hat{\beta} \right] - \beta $$

---

**Respuesta uno: Sesgo.**

.pull-left[

**Estimador insesgado:** $\mathop{\boldsymbol{E}}\left[ \hat{\beta} \right] = \beta$

```{R, unbiased pdf, echo = F, dev = "svg"}
ggplot(data = tmp, aes(x, y)) +
geom_polygon(fill = red_pink, alpha = 0.9) +
geom_hline(yintercept = 0, color = "black") +
geom_vline(xintercept = 0, size = 1, linetype = "dashed") +
scale_x_continuous(breaks = 0, labels = TeX("$\\beta$")) +
theme_simple +
theme(axis.text.x = element_text(size = 40))
```

]

--

.pull-right[

**Estimador sesgado:** $\mathop{\boldsymbol{E}}\left[ \hat{\beta} \right] \neq \beta$

```{R, biased pdf, echo = F, dev = "svg"}
ggplot(data = tmp, aes(x, y)) +
geom_polygon(aes(x = x + 2), fill = "darkslategray", alpha = 0.9) +
geom_hline(yintercept = 0, color = "black") +
geom_vline(xintercept = 0, size = 1, linetype = "dashed") +
scale_x_continuous(breaks = 0, labels = TeX("$\\beta$")) +
theme_simple +
theme(axis.text.x = element_text(size = 40))
```

]

---

**Respuesta dos: Varianza.**

Las tendencias centrales (medias) de distribuciones en competencia no son las 煤nicas cosas que importan. Tambi茅n nos preocupamos por la **varianza** de un estimador.

$$ \mathop{\text{Var}} \left( \hat{\beta} \right) = \mathop{\boldsymbol{E}}\left[ \left( \hat{\beta} - \mathop{\boldsymbol{E}}\left[ \hat{\beta} \right] \right)^2 \right] $$

Los estimadores de varianza m谩s baja significan que obtenemos estimaciones m谩s cercanas a la media en cada muestra.

---
count: false

**Respuesta dos: Varianza.**

```{R, variance pdf, echo = F, dev = "svg", fig.height = 5}
d4 <- tibble(x = seq(-7.5, 7.5, 0.01), y = dnorm(x, mean = 0, sd = 1)) %>%
  rbind(., tibble(x = seq(7.5, -7.5, -0.01), y = 0))
d5 <- tibble(x = seq(-7.5, 7.5, 0.01), y = dnorm(x, mean = 0, sd = 2)) %>%
  rbind(., tibble(x = seq(7.5, -7.5, -0.01), y = 0))
ggplot() +
geom_polygon(data = d4, aes(x, y), fill = red_pink, alpha = 0.9) +
geom_polygon(data = d5, aes(x, y), fill = "darkslategray", alpha = 0.8) +
geom_hline(yintercept = 0, color = "black") +
geom_vline(xintercept = 0, size = 1, linetype = "dashed") +
scale_x_continuous(breaks = 0, labels = TeX("$\\beta$")) +
theme_simple +
theme(axis.text.x = element_text(size = 20))
```

---

**Respuesta uno: Sesgo.**

**Respuesta dos: Varianza.**

**S煤tilmente:** El trade-off sesgo-varianza.

驴Deber铆amos estar dispuestos a tomar un poco de sesgo para reducir la varianza?

En econometr铆a, generalmente nos apegamos a estimadores insesgados (o consistentes). Pero otras disciplinas (especialmente las ciencias de la computaci贸n) piensan un poco m谩s en esta compensaci贸n.

---
layout: false

# El trade-off sesgo-varianza.

```{R, variance bias, echo = F, dev = "svg"}
d4 <- tibble(x = seq(-7.5, 7.5, 0.01), y = dnorm(x, mean = 0.3, sd = 1)) %>%
  rbind(., tibble(x = seq(7.5, -7.5, -0.01), y = 0))
d5 <- tibble(x = seq(-7.5, 7.5, 0.01), y = dnorm(x, mean = 0, sd = 2)) %>%
  rbind(., tibble(x = seq(7.5, -7.5, -0.01), y = 0))
ggplot() +
geom_polygon(data = d4, aes(x, y), fill = red_pink, alpha = 0.9) +
geom_polygon(data = d5, aes(x, y), fill = "darkslategray", alpha = 0.8) +
geom_hline(yintercept = 0, color = "black") +
geom_vline(xintercept = 0, size = 1, linetype = "dashed") +
scale_x_continuous(breaks = 0, labels = TeX("$\\beta$")) +
theme_simple +
theme(axis.text.x = element_text(size = 20))
```

---
# MCO: Supuestos y propiedades

## Propiedades

Como ya habr谩n adivinado,

- MCO es **insesgado**.
- MCO tiene la **varianza m铆nima** de todos los estimadores lineales insesgados.

---
# MCO: Supuestos y propiedades

## Propiedades

Pero... estas (muy buenas) propiedades dependen de un conjunto de supuestos:

1. La relaci贸n de poblaci贸n es lineal en par谩metros con una perturbaci贸n aditiva.

2. Nuestra variable $X$ es **ex贸gena**, _es decir_, $\mathop{\boldsymbol{E}}\left[u \mid X \right] = 0$.

3. La variable $X$ tiene variaci贸n. Y si hay m煤ltiples variables explicativas, no son perfectamente colineales.

4. Las perturbaciones de la poblaci贸n $u_i$ se distribuyen de forma independiente e id茅ntica como variables aleatorias normales con una media de cero $\left(\mathop{\boldsymbol{E}} \left[u \right] = 0 \right) $ y varianza $\sigma^2$ (_es decir_, $\mathop{\boldsymbol{E}} \left[u^2\right] = \sigma^2$). Independientemente distribuidos y significan cero conjuntamente implican $\mathop{\boldsymbol{E}} \left[u_i u_j\right] = 0$ para cualquier $i \neq j$.

---
# MCO: Supuestos y propiedades

## Supuestos

Diferentes supuestos garantizan diferentes propiedades:

- Supuestos (1), (2), y (3) hace al insesgado MCO.
- Supuesto (4) nos da un estimador insesgado para la varianza de nuestro estimador MCO.

Durante nuestro curso, discutiremos las muchas formas en que la vida real puede **violar estas suposiciones**. Por ejemplo:

- Relaciones no lineales en nuestros par谩metros/perturbaciones (o errores de especificaci贸n).
- Perturbaciones que no se distribuyen de forma id茅ntica y/o no son independientes.
- Violaciones de exogeneidad (especialmente sesgo de variable omitida).

---
# MCO: Supuestos y propiedades

## Expectativa condicional

Para muchas aplicaciones, nuestro supuesto m谩s importante es **exogeneidad**, _es decir_,
$$
\begin{align}
  \mathop{E}\left[ u \mid X \right] = 0
\end{align}
$$

驴pero qu茅 significa esto?

--

Una forma de pensar en esta definici贸n:

> Para *cualquier* valor de $X$, la media de los residuos debe ser cero.

- _Por ejemplo_, $\mathop{E}\left[ u \mid X=1 \right]=0$ *y* $\mathop{E}\left[ u \mid X=100 \right]=0$

- _Por ejemplo_, $\mathop{E}\left[ u \mid X_2=\text{Mujer} \right]=0$ *y* $\mathop{E}\left[ u \mid X_2=\text{Var贸n} \right]=0$

- Aviso: $\mathop{E}\left[ u \mid X \right]=0$ es m谩s restrictiva que $\mathop{E}\left[ u \right]=0$
---
layout: false
class: clear, middle

Gr谩ficamente...
---
exclude: true

```{R, conditional_expectation_setup, include = F, cache = T}

# Setup ----------------------------------------------------------------------------------
  # Options
  options(stringsAsFactors = F)
  # Packages
  library(pacman)
  p_load(ggridges)

# Data work ------------------------------------------------------------------------------
  # Set seed
  set.seed(1989)
  # Sample size
  n <- 1e5
  # Exogenous
  e_good <- tibble(
    x = runif(n = n, min = -4, max = 4),
    e = rnorm(n)
  ) %>% mutate(x = round(x))
  # Endogenous
  e_bad <- tibble(
    x = runif(n = n, min = -4, max = 4),
    e = rnorm(n) + 0.5 * x
  ) %>% mutate(x = round(x))

# Figures: Joint densities ---------------------------------------------------------------
  # The joint plot: good
  joint_good <- ggplot(data = e_good, aes(x = e)) +
    geom_density() +
    theme_pander()
  # The joint plot: bad
  joint_bad <- ggplot(data = e_bad, aes(x = e)) +
    geom_density() +
    theme_pander()

# Figures: Conditional densities ---------------------------------------------------------
  cond_good <- ggplot(data = e_good, aes(x = e, y = as.factor(x))) +
    geom_density_ridges_gradient(
      aes(fill = ..x..),
      color = "white",
      scale = 2.5,
      size = 0.2
    ) +
    # geom_vline(xintercept = 0, alpha = 0.3) +
    scale_fill_viridis(option = "magma") +
    xlab("u") +
    ylab("X") +
    theme_pander(base_family = "Fira Sans Book", base_size = 18) +
    theme(
      legend.position = "none",
      axis.title.y = element_text(angle = 0, vjust = 0.5, family = "MathJax_Math", size = 22),
      axis.title.x = element_text(family = "MathJax_Math", size = 22)
    )
  cond_bad <- ggplot(data = e_bad, aes(x = e, y = as.factor(x))) +
    geom_density_ridges_gradient(
      aes(fill = ..x..),
      color = "white",
      scale = 2.5,
      size = 0.2
    ) +
    # geom_vline(xintercept = 0, alpha = 0.3) +
    scale_fill_viridis(option = "magma") +
    xlab("u") +
    ylab("X") +
    theme_pander(base_family = "Fira Sans Book", base_size = 18) +
    theme(
      legend.position = "none",
      axis.title.y = element_text(angle = 0, vjust = 0.5, family = "MathJax_Math", size = 22),
      axis.title.x = element_text(family = "MathJax_Math", size = 22)
    )
```
---
class: clear

La exogeneidad v谩lida, _es decir_, $\mathop{E}\left[ u \mid X \right] = 0$

```{R, ex_good_exog, echo = F, dev = "svg"}
cond_good
```
---
class: clear

La exogeneidad inv谩lida, _es decir_, $\mathop{E}\left[ u \mid X \right] \neq 0$

```{R, ex_bad_exog, echo = F, dev = "svg"}
cond_bad
```


---
layout: false
class: inverse, middle
# Incertidumbre e inferencia

---
layout: true
# Incertidumbre e inferencia

---

## 驴Hay algo m谩s?

Hasta este punto, sabemos que MCO tiene algunas propiedades interesantes, y sabemos c贸mo estimar un coeficiente de pendiente de intersecci贸n y a trav茅s de MCO.

Nuestro flujo de trabajo actual:
- Obtener datos (puntos con valores $x$ y $y$)
- Regresar $y$ en $x$
- Trace la l铆nea MCO (_es decir_, $\hat {y} = \hat {\beta}_0 + \hat{\beta}_1 $)
- 驴Hecho?

Pero, 驴c贸mo podemos **aprender** algo de este ejercicio?
---

## Hay m谩s

Pero, 驴c贸mo podemos **aprender** algo de este ejercicio?

- Con base en nuestro valor de $\hat{\beta}_1$, 驴podemos descartar valores hipotetizados previamente?
- 驴Qu茅 confianza debemos tener en la precisi贸n de nuestras estimaciones?
- 驴Qu茅 tan bien explica nuestro modelo la variaci贸n que observamos en $y$?

Necesitamos poder lidiar con la incertidumbre. Ingresa en escena: **La inferencia.**

---
layout: true
# Incertidumbre e inferencia
## Aprendiendo de nuestros errores

---

Como se帽al贸 nuestra simulaci贸n anterior, nuestro problema con la **incertidumbre** es que no sabemos si nuestra estimaci贸n muestral est谩 *cerca* o *lejos* del par谩metro de poblaci贸n desconocido. <sup> .pink[] </sup>

Sin embargo, no todo est谩 perdido. Podemos usar los errores $\left (e_i = y_i - \hat{y}_i \right)$ para tener una idea de qu茅 tan bien nuestro modelo explica la variaci贸n observada en $y$.

Cuando nuestro modelo parece estar haciendo un trabajo "agradable", es posible que tengamos un poco m谩s de confianza al usarlo para conocer la relaci贸n entre $y$ y $x$.

Ahora solo tenemos que formalizar lo que realmente significa un "buen trabajo".

.note[.pink []: Excepto cuando corremos la simulaci贸n nosotros mismos, por eso nos gustan las simulaciones ]

---

En primer lugar, estimaremos la varianza de $u_i$ (recuerde: $\mathop{\text{Var}} \left (u_i \right) = \sigma^ 2$) usando nuestros errores al cuadrado, _es decir_,

$$s^2 = \dfrac{\sum_i e_i^2} {n - k}$$

donde $k$ da el n煤mero de t茅rminos de pendiente y intersecciones que estimamos (_por ejemplo_, $\beta_0$ y $\beta_1$ dar铆a $k = 2$).

$s^2$ es un estimador insesgado de $\ sigma^2$.

---

Luego demostramos que la varianza de $\hat {\beta}_1$ (para regresi贸n lineal simple) es

$$\mathop {\text {Var}} \left (\hat {\beta} _1 \right) = \dfrac {s^2} {\sum_i \left (x_i - \overline {x} \right)^ 2}$$

lo que muestra que la varianza de nuestro estimador de pendiente:

1. aumenta a medida que nuestras perturbaciones se vuelven m谩s ruidosas
2. disminuye a medida que aumenta la varianza de $x$

---

*M谩s com煤nmente:* El **error est谩ndar** de $\hat{\beta}_1$

$$ \mathop{\hat{\text{EE}}} \left( \hat{\beta}_1 \right) = \sqrt{\dfrac{s^2}{\sum_i \left( x_i - \overline{x} \right)^2}} $$

*Recuerden:* El error est谩ndar de un estimador es la desviaci贸n est谩ndar de la distribuci贸n del estimador.

---

El error est谩ndar en .mono[R]'s `lm`, se ve as铆:

```{R, se}
tidy(lm(y ~ x, pop_df))
```


---
layout: false
class: inverse, middle
# Interpretando coeficientes

---
layout: true
# Interpretando coeficientes

---
## Variables continuas

Consideramos la siguiente relaci贸n:

$$ \text{salario}_i = \beta_0 + \beta_1 \, \text{educaci贸n}_i + u_i $$

donde

- $\text{salario}_i$ es una variable continua que mide el salario de cada individuo
- $\text{educaci贸n}_i$ es una variable continua que mide los a帽os de educaci贸n de cada persona

--

**Interpretaciones**

- $\beta_0$: es el intercepto de $y$, _es decir_, el $\text{salario}$ cuando $\text{educaci贸n} = 0$
- $\beta_1$: el aumento esperado en el $\text{salario}$ para un aumento unitario de $\text{educaci贸n}$

---
## Variables continuas

Considerando el siguiente modelo:

$$ y = \beta_0 + \beta_1 \, x + u $$

Diferencio el modelo:

$$ \dfrac{dy}{dx} = \beta_1 $$

_Es decir_, la pendiente nos dice el aumento esperado en la variable dependiente para un aumento en una unidad de la variable explicativa, **manteniendo todas las dem谩s variables constantes** (*ceteris paribus*).
---
## Variables binarias

Considero la relaci贸n:

$$ \text{salario}_i = \beta_0 + \beta_1 \, \text{mujer}_i + u_i $$

donde

- $\text{salario}_i$ es una variable continua que mide el salario de cada individuo
- $\text{mujer}_i$ es un variable binaria que toma el valor $1$ cuando $i$ es mujer

--

**Interpretaciones**

- $\beta_0$: el $\text{salario}$ esperado para los hombres (cuando $\text{mujer} = 0$)
- $\beta_1$: la diferencia esperada en el $\text{salario}$ entre hombres y mujeres
- $\beta_0 + \beta_1$: el $\text{salario}$ esperado para mujeres

---
## Variables binarias

Derivaci贸n:

$$
\begin{aligned}
 \mathop{\boldsymbol{E}}\left[ \text{salario} | \text{hombre} \right] &=
 \mathop{\boldsymbol{E}}\left[ \beta_0 + \beta_1\times 0 + u_i \right] \\
 &= \mathop{\boldsymbol{E}}\left[ \beta_0 + 0 + u_i \right] \\
 &= \beta_0
\end{aligned}
$$

--

$$
\begin{aligned}
 \mathop{\boldsymbol{E}}\left[ \text{salario} | \text{mujer} \right] &=
 \mathop{\boldsymbol{E}}\left[ \beta_0 + \beta_1\times 1 + u_i \right] \\
 &= \mathop{\boldsymbol{E}}\left[ \beta_0 + \beta_1 + u_i \right] \\
 &= \beta_0 + \beta_1
\end{aligned}
$$

--

**Nota:** Si no hay variables adicionales de control, entonces $\hat{\beta}_1$ equivale a la diferencia entre las medias de los grupos, _en este caso_, $\overline{x}_\text{mujer} - \overline{x}_\text{hombre}$.

--

**Nota<sub>2</sub>:** El *manteniendo todo lo dem谩s constante* tambi茅n aplica en regresiones que tienen variables binarias.

---
## Variables binarias

$y_i = \beta_0 + \beta_1 x_i + u_i$ para la variable binaria $x_i = \{\color{#314f4f}{0}, \, \color{#e64173}{1}\}$

```{R, cat data, include = F}
# Set seed
set.seed(1235)
# Sample size
n <- 5e3
# Generate data
cat_df <- tibble(
  x = sample(x = c(0, 1), size = n, replace = T),
  y = 3 + 7 * x + rnorm(n, sd = 2)
)
# Regression
cat_reg <- lm(y ~ x, data = cat_df)
```

```{R, dat plot 1, echo = F, dev = "svg", fig.height = 5.5}
set.seed(12345)
ggplot(data = cat_df, aes(x = x, y = y, color = as.factor(x))) +
geom_jitter(width = 0.3, size = 1.5, alpha = 0.5) +
scale_color_manual(values = c("darkslategrey", red_pink)) +
theme_empty
```

---
## Variables binarias

$y_i = \beta_0 + \beta_1 x_i + u_i$ para la variable binarias $x_i = \{\color{#314f4f}{0}, \, \color{#e64173}{1}\}$

```{R, dat plot 2, echo = F, dev = "svg", fig.height = 5.5}
set.seed(12345)
ggplot(data = cat_df, aes(x = x, y = y, color = as.factor(x))) +
geom_jitter(width = 0.3, size = 1.5, alpha = 0.5) +
scale_color_manual(values = c("darkslategrey", red_pink)) +
geom_hline(yintercept = cat_reg$coefficients[1], size = 1, color = "darkslategrey") +
geom_hline(yintercept = cat_reg$coefficients[1] + cat_reg$coefficients[2], size = 1, color = red_pink) +
annotate(
  geom = "text",
  x = 0.5,
  y = -1 + cat_reg$coefficients[1],
  label = TeX("$\\hat{\\beta}_0 = \\bar{\\mathrm{Grupo}_0}$"),
  size = 7
) +
annotate(
  geom = "text",
  x = 0.5,
  y = 1 + cat_reg$coefficients[1] + cat_reg$coefficients[2],
  label = TeX("$\\hat{\\beta}_0 + \\hat{\\beta}_1 = \\bar{\\mathrm{Grupo}_1}$"),
  size = 7,
  color = red_pink
) +
theme_empty
```
---
## Interacciones

Las interacciones permiten que el efecto de una variable cambie seg煤n el nivel de otra variable.

** Ejemplos **

1. 驴Cambia el efecto de la escolarizaci贸n en el salario por sexo?

2. 驴El efecto del g茅nero en el salario cambia seg煤n la etnia?

3. 驴Cambia el efecto de la escolarizaci贸n en el salario seg煤n la experiencia?
---

## Interacciones

Anteriormente, consider谩bamos un modelo que permit铆a a mujeres y hombres tener salarios diferentes, pero el modelo asum铆a que el efecto de la escuela sobre el salario era el mismo para todos:

$$ \text{salario}_i = \beta_0 + \beta_1 \, \text{educaci贸n}_i + \beta_2 \, \text{mujer}_i + u_i $$

pero tambi茅n podemos permitir que el efecto de la escuela var铆e seg煤n el sexo:

$$ \text{salario}_i = \beta_0 + \beta_1 \, \text{educaci贸n}_i + \beta_2 \, \text{mujer}_i + \beta_3 \, \text{educaci贸n}_i\times\text{mujer}_i + u_i $$

---

## Interacciones

El modelo donde la escolarizaci贸n tiene el mismo efecto para todos (**<font color="#e64173">M</font>** y **<font color="#314f4f">H</font>**):

```{R, int data, include = F, cache = T}
# Set seed
set.seed(1989)
# Sample size
n <- 1e3
# Parameters
beta0 <- 20; beta1 <- 0.5; beta2 <- 10; beta3 <- 3
# Dataset
int_df <- tibble(
  hombre = sample(x = c(F, T), size = n, replace = T),
  educaci贸n = runif(n, 3, 9) - 3 * hombre,
  salario = beta0 + beta1 * educaci贸n + beta2 * hombre + rnorm(n, sd = 7) + beta3 * hombre * educaci贸n
)
reg_noint <- lm(salario ~ educaci贸n + hombre, int_df)
reg_int <- lm(salario ~ educaci贸n + hombre + educaci贸n:hombre, int_df)
```

```{R, int plot 1, echo = F, dev = "svg", fig.height = 5.5}
ggplot(data = int_df, aes(x = educaci贸n, y = salario)) +
geom_point(aes(color = hombre, shape = hombre), size = 2.5) +
geom_hline(yintercept = 0) +
geom_vline(xintercept = 0) +
geom_abline(
  intercept = reg_noint$coefficients[1] + reg_noint$coefficients[3],
  slope = reg_noint$coefficients[2],
  color = "darkslategrey", size = 1, alpha = 0.8
) +
geom_abline(
  intercept = reg_noint$coefficients[1],
  slope = reg_noint$coefficients[2],
  color = red_pink, size = 1, alpha = 0.8
) +
xlab("educaci贸n") +
ylab("salario") +
theme_empty +
theme(
  axis.title = element_text(size = 18),
  plot.margin = structure(c(0, 0, 0.1, 0), unit = "lines", valid.unit = 3L, class = "unit"),
) +
scale_color_manual("", values = c(red_pink, "darkslategrey"), labels = c("Mujer", "Hombre")) +
scale_shape_manual("", values = c(16, 1), labels = c("Mujer", "Hombre"))
```

---

## Interacciones

El modelo donde el efecto de la educaci贸n puede variar por sexo (**<font color="#e64173">M</font>** y **<font color="#314f4f">H</font>**):

```{R, int plot 2, echo = F, dev = "svg", fig.height = 5.5}
ggplot(data = int_df, aes(x = educaci贸n, y = salario)) +
geom_point(aes(color = hombre, shape = hombre), size = 2.5) +
geom_hline(yintercept = 0) +
geom_vline(xintercept = 0) +
geom_abline(
  intercept = reg_noint$coefficients[1] + reg_noint$coefficients[3],
  slope = reg_noint$coefficients[2],
  color = "darkslategrey", size = 0.75, alpha = 0.2
) +
geom_abline(
  intercept = reg_noint$coefficients[1],
  slope = reg_noint$coefficients[2],
  color = red_pink, size = 0.75, alpha = 0.2
) +
geom_abline(
  intercept = reg_int$coefficients[1] + reg_int$coefficients[3],
  slope = reg_int$coefficients[2] + reg_int$coefficients[4],
  color = "darkslategrey", size = 1, alpha = 0.8
) +
geom_abline(
  intercept = reg_int$coefficients[1],
  slope = reg_int$coefficients[2],
  color = red_pink, size = 1, alpha = 0.8
) +
xlab("educaci贸n") +
ylab("salario") +
theme_empty +
theme(
  axis.title = element_text(size = 18),
  plot.margin = structure(c(0, 0, 0.1, 0), unit = "lines", valid.unit = 3L, class = "unit"),
) +
scale_color_manual("", values = c(red_pink, "darkslategrey"), labels = c("Mujer", "Hombre")) +
scale_shape_manual("", values = c(16, 1), labels = c("Mujer", "Hombre"))
```

<!-- --- -->
<!-- ## Interactions -->

<!-- Interpreting coefficients can be a little tricky with interactions, but the key<sup>.pink[]</sup> is to carefully work through the math. -->

<!-- .footnote[.pink[] As is often the case with econometrics.] -->

<!-- $$ \text{salario}_i = \beta_0 + \beta_1 \, \text{educaci贸n}_i + \beta_2 \, \text{Female}_i + \beta_3 \, \text{educaci贸n}_i\times\text{Female}_i + u_i $$ -->

<!-- Expected returns for an additional year of educaci贸ning for women: -->

<!-- $$ -->
<!-- \begin{aligned} -->
<!--  \mathop{\boldsymbol{E}}\left[ \text{salario}_i | \text{Female} \land \text{educaci贸n} = \ell + 1 \right] - -->
<!--     \mathop{\boldsymbol{E}}\left[ \text{salario}_i | \text{Female} \land \text{educaci贸n} = \ell \right] &= \\ -->
<!--  \mathop{\boldsymbol{E}}\left[ \beta_0 + \beta_1 (\ell+1) + \beta_2 + \beta_3 (\ell + 1) + u_i \right] - -->
<!--     \mathop{\boldsymbol{E}}\left[ \beta_0 + \beta_1 \ell + \beta_2 + \beta_3 \ell + u_i  \right] &= \\ -->
<!--  \beta_1 + \beta_3 -->
<!-- \end{aligned} -->
<!-- $$ -->

<!-- -- -->

<!-- Similarly, $\beta_1$ gives the expected return to an additional year of educaci贸ning for men. Thus, $\beta_3$ gives the **difference in the returns to educaci贸ning** for women and men. -->

---
## Especificaci贸n log-lineal

En econom铆a, con frecuencia se emplean variables explicativas en logaritmos, por ejemplo,

$$ \log(\text{salario}_i) = \beta_0 + \beta_1 \, \text{educaci贸n}_i + u_i $$


Esta especificaci贸n cambia nuestra interpretaci贸n de los coeficientes de pendiente.

**Interpretaci贸n**

- Un aumento de una unidad en nuestra variable explicativa aumenta la variable de resultado en aproximadamente $\beta_1$ veces $100$ por ciento.

- *Ejemplo:* Un a帽o adicional de educaci贸n aumenta el salario en aproximadamente un 3 por ciento (para $\beta_1 = 0.03$).

<!-- --- -->
<!-- # Interpretando coeficientes -->
<!-- ## Especificaci贸n log-lineal -->

<!-- **Derivaci贸n** -->

<!-- Considerando el modelo log-lineal: -->

<!-- $$ \log(y) = \beta_0 + \beta_1 \, x + u $$ -->

<!-- y diferenciando -->

<!-- $$ \dfrac{dy}{y} = \beta_1 dx $$ -->

<!-- Entonces un cambio marginal en $x$ , $\text{dx}$ , lleva a un aumento $\beta_1 dx$ **cambio porcentual** en $y$. -->

---
## Especificaci贸n log-lineal

```{R, log linear plot, echo = F, cache = T, dev = "svg", fig.height = 6}
# Set seed
set.seed(1234)
# Sample size
n <- 1e3
# Generate data
ll_df <- tibble(
  x = runif(n, 0, 3),
  y = exp(-100 + 0.75 * x + rnorm(n, sd = 0.5))
)
# Plot
ggplot(data = ll_df, aes(x = x, y = y)) +
geom_hline(yintercept = 0) +
geom_vline(xintercept = 0) +
geom_point(size = 3, color = "darkslategrey", alpha = 0.5) +
geom_smooth(color = red_pink, se = F) +
xlab("x") +
ylab("y") +
theme_axes
```

---
## Especificaci贸n log-log 

De manera similar, los econometristas emplean con frecuencia modelos log-log, en los que y se est谩 en logaritmos y al menos una variable explicativa, tambi茅n:

$$ \log(\text{salario}_i) = \beta_0 + \beta_1 \, \log(\text{educaci贸n}_i) + u_i $$

**Interpretaci贸n:**

- Un aumento del uno por ciento en $x$ dar谩 lugar a un cambio porcentual de $\beta_1$ en $y$.
- Se interpreta como una elasticidad.

---
## Especificaci贸n log-log 

**Derivaci贸n**

Consideramos el siguiente modelo log-log:

$$ \log(y) = \beta_0 + \beta_1 \, \log(x) + u $$
y diferenciamos

$$ \dfrac{dy}{y} = \beta_1 \dfrac{dx}{x} $$

que dice que para un aumento del uno por ciento en $x$, veremos un aumento del $\beta_1$ por ciento en $y$. Como elasticidad:

$$ \dfrac{dy}{dx} \dfrac{x}{y} = \beta_1 $$

---
## Log-lineal con variable binaria

** Nota: ** Si tenemos un modelo log-lineal con una variable binaria, la interpretaci贸n del coeficiente de esa variable cambia.

Consideramos:

$$\log(y_i) = \beta_0 + \beta_1 x_1 + u_i$$

siendo $x_1$ una variable binaria.

  
La interpretaci贸n de $\beta_1$ ahora es:

- Cuando $x_1$ cambia de 0 a 1, $y$ cambiar谩 en $100\times\left(e ^ {\beta_1} -1 \right)$ por ciento.

---
##Resumen

| Modelo      | Interpretaci贸n                                                                                            |
|-------------|-----------------------------------------------------------------------------------------------------------|
| Nivel-nivel | Incremento de unidades en "y" cuando aumenta 1 unidad la "x" (ambas en sus unidades de medida originales) |
| Log-nivel   | $\beta$*100 (incremento porcentual de "y" cuando aumenta una unidad la "x")                                      |
| Nivel-log   | $\beta$/100 (incremento en unidades de "y" cuando aumenta un 1% la "x")                                          |
| Log-log     | Incremento porcentual de "y" cuando aumenta un 1% la "x"                                                  |

